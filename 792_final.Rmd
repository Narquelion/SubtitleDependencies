---
output:
  pdf_document:
    latex_engine: xelatex
    keep_tex: yes
    fig_caption: yes
    number_sections: true
  html_document: default
always_allow_html: yes
geometry: margin=1in
documentclass: article
header-includes:
  - \setlength{\parindent}{2em}
  - \setlength{\parskip}{0em}
  - \usepackage{setspace, booktabs, minted, fancyhdr, titling, fontspec}
  - \setromanfont[SmallCapsFont={Baskerville Small Caps SSi}]{Baskerville}
  - \usepackage[backend=biber, authordate, natbib, giveninits=true]{biblatex-chicago}
  - \bibliography{792_final}
---

```{r setup, include=FALSE}
library(kableExtra)
library(dplyr)
library(ggplot2)
library(ghibli)

options(tinytex.engine_args = '-shell-escape')
knitr::opts_chunk$set(echo = TRUE)
```

```{r yuuka_data, include=FALSE}
df.yuuka.observed <- data.frame(read.table("dependencies/ja/YukaKinoshita/YukaKinoshita_observed_dependencies.csv",
                                              header=TRUE, sep=","))
df.yuuka.optimal  <- data.frame(read.table("dependencies/ja/YukaKinoshita/YukaKinoshita_optimal_dependencies.csv",
                                              header=TRUE, sep=","))
df.yuuka.random   <- data.frame(read.table("dependencies/ja/YukaKinoshita/YukaKinoshita_random_dependencies.csv",
                                            header=TRUE, sep=","))
df.yuuka <- bind_rows(list("Observed"=df.yuuka.observed, "Optimal"=df.yuuka.optimal, 
                            "Random"=df.yuuka.random),  .id = 'baseline')
```

```{r hikakin_data, include=FALSE}
df.hikakin.observed <- data.frame(read.table("dependencies/ja/HikakinTV/HikakinTV_observed_dependencies.csv", 
                                             header=TRUE, sep=","))
df.hikakin.optimal  <- data.frame(read.table("dependencies/ja/HikakinTV/HikakinTV_optimal_dependencies.csv",  
                                             header=TRUE, sep=","))
df.hikakin.random   <- data.frame(read.table("dependencies/ja/HikakinTV/HikakinTV_random_dependencies.csv",   
                                             header=TRUE, sep=","))
df.hikakin <- bind_rows(list("Observed"=df.hikakin.observed, "Optimal"=df.hikakin.optimal, 
                             "Random"=df.hikakin.random),  .id = 'baseline')
```

```{r fischers_data, include=FALSE}
df.fischers.observed <- data.frame(read.table("dependencies/ja/Fischers/Fischers_observed_dependencies.csv",
                                                  header=TRUE, sep=","))
df.fischers.optimal  <- data.frame(read.table("dependencies/ja/Fischers/Fischers_optimal_dependencies.csv",
                                                  header=TRUE, sep=","))
df.fischers.random   <- data.frame(read.table("dependencies/ja/Fischers/Fischers_random_dependencies.csv", 
                                                  header=TRUE, sep=","))
df.fischers <- bind_rows(list("Observed"=df.fischers.observed, "Optimal"=df.fischers.optimal,
                                  "Random"=df.fischers.random),  .id = 'baseline')
```

```{r hajime_data, include=FALSE}
df.hajime.observed <- data.frame(read.table("dependencies/ja/HajimeShachoo/HajimeShachoo_observed_dependencies.csv",
                                                header=TRUE, sep=","))
df.hajime.optimal  <- data.frame(read.table("dependencies/ja/HajimeShachoo/HajimeShachoo_optimal_dependencies.csv",
                                                header=TRUE, sep=","))
df.hajime.random   <- data.frame(read.table("dependencies/ja/HajimeShachoo/HajimeShachoo_random_dependencies.csv",
                                                header=TRUE, sep=","))
df.hajime <- bind_rows(list("Observed"=df.hajime.observed, "Optimal"=df.hajime.optimal, 
                                "Random"=df.hajime.random),  .id = 'baseline')
```

```{r tokai_data, include=FALSE}
df.tokai.observed <- data.frame(read.table("dependencies/ja/TokaiOnAir/TokaiOnAir_observed_dependencies.csv",
                                               header=TRUE, sep=","))
df.tokai.optimal  <- data.frame(read.table("dependencies/ja/TokaiOnAir/TokaiOnAir_optimal_dependencies.csv",
                                               header=TRUE, sep=","))
df.tokai.random   <- data.frame(read.table("dependencies/ja/TokaiOnAir/TokaiOnAir_random_dependencies.csv", 
                                               header=TRUE, sep=","))
df.tokai <- bind_rows(list("Observed"=df.tokai.observed, "Optimal"=df.tokai.optimal, 
                               "Random"=df.tokai.random),  .id = 'baseline')
```

```{r jp_all_setup, include=FALSE}
df.all.jp <- bind_rows(list("Yuuka Kinoshita"=df.yuuka, 
                             "Fischer's"=df.fischers,
                             "Hajime Shacho"=df.hajime, 
                             "HikakinTV"=df.hikakin,
                             "TokaiOnAir"=df.tokai), .id = 'channel')

# Calculate squared sentence length to match Futrell et al.: better predictor than raw length
df.all.jp$sent_len_sq   <- df.all.jp$total_length * df.all.jp$total_length

# Create index variables: ri = 1 if random; 0 else, mi = 1 if optimal; 0 else
df.all.jp$baseline <- factor(df.all.jp$baseline, levels=c("Random", "Optimal", "Observed")) 
df.all.jp <- df.all.jp %>% mutate(ri = as.integer(baseline == "Random"), mi = as.integer(baseline == "Optimal")) %>% filter(dep_length > 5 & dep_length < 300)

# Observed deps for density hexagons
df.all.jp.observed <- df.all.jp %>% filter(baseline=="Observed")
  
# Average deps for fitted scatter plots
df.all.jp.avg <- df.all.jp %>% 
  group_by(channel, baseline, total_length) %>% summarize("avg_length"=mean(dep_length))
df.all.jp.observed.avg <- df.all.jp %>% filter(baseline=="Observed")
```

```{r adam_data, include=FALSE}
df.adam.observed <- data.frame(read.table("dependencies/ru/AdamThomasMoran/AdamThomasMoran_observed_dependencies.csv",
                                              header=TRUE, sep=","))
df.adam.optimal  <- data.frame(read.table("dependencies/ru/AdamThomasMoran/AdamThomasMoran_optimal_dependencies.csv",
                                              header=TRUE, sep=","))
df.adam.random   <- data.frame(read.table("dependencies/ru/AdamThomasMoran/AdamThomasMoran_random_dependencies.csv",
                                            header=TRUE, sep=","))
df.adam <- bind_rows(list("Observed"=df.adam.observed, "Optimal"=df.adam.optimal, 
                            "Random"=df.adam.random),  .id = 'baseline')
```

```{r ntv_data, include=FALSE}
df.ntv.observed <- data.frame(read.table("dependencies/ru/NTV/NTV_observed_dependencies.csv",
                                              header=TRUE, sep=","))
df.ntv.optimal  <- data.frame(read.table("dependencies/ru/NTV/NTV_optimal_dependencies.csv",
                                              header=TRUE, sep=","))
df.ntv.random   <- data.frame(read.table("dependencies/ru/NTV/NTV_random_dependencies.csv",
                                            header=TRUE, sep=","))
df.ntv <- bind_rows(list("Observed"=df.ntv.observed, "Optimal"=df.ntv.optimal, 
                            "Random"=df.ntv.random),  .id = 'baseline')
```

```{r brian_data, include=FALSE}
df.TheBrianMaps.observed <- data.frame(read.table("dependencies/ru/TheBrianMaps/TheBrianMaps_observed_dependencies.csv",
                                              header=TRUE, sep=","))
df.TheBrianMaps.optimal  <- data.frame(read.table("dependencies/ru/TheBrianMaps/TheBrianMaps_optimal_dependencies.csv",
                                              header=TRUE, sep=","))
df.TheBrianMaps.random   <- data.frame(read.table("dependencies/ru/TheBrianMaps/TheBrianMaps_random_dependencies.csv",
                                            header=TRUE, sep=","))
df.TheBrianMaps <- bind_rows(list("Observed"=df.TheBrianMaps.observed, "Optimal"=df.TheBrianMaps.optimal, 
                            "Random"=df.TheBrianMaps.random),  .id = 'baseline')
```

```{r horosho_data, include=FALSE}
df.ThisIsHorosho.observed <- data.frame(read.table("dependencies/ru/ThisIsHorosho/ThisIsHorosho_observed_dependencies.csv",
                                              header=TRUE, sep=","))
df.ThisIsHorosho.optimal  <- data.frame(read.table("dependencies/ru/ThisIsHorosho/ThisIsHorosho_optimal_dependencies.csv",
                                              header=TRUE, sep=","))
df.ThisIsHorosho.random   <- data.frame(read.table("dependencies/ru/ThisIsHorosho/ThisIsHorosho_random_dependencies.csv",
                                            header=TRUE, sep=","))
df.ThisIsHorosho <- bind_rows(list("Observed"=df.ThisIsHorosho.observed, "Optimal"=df.ThisIsHorosho.optimal, 
                            "Random"=df.ThisIsHorosho.random),  .id = 'baseline')
```

```{r wylsacom_data, include=FALSE}
df.Wylsacom.observed <- data.frame(read.table("dependencies/ru/Wylsacom/Wylsacom_observed_dependencies.csv",
                                              header=TRUE, sep=","))
df.Wylsacom.optimal  <- data.frame(read.table("dependencies/ru/Wylsacom/Wylsacom_optimal_dependencies.csv",
                                              header=TRUE, sep=","))
df.Wylsacom.random   <- data.frame(read.table("dependencies/ru/Wylsacom/Wylsacom_random_dependencies.csv",
                                            header=TRUE, sep=","))
df.Wylsacom <- bind_rows(list("Observed"=df.Wylsacom.observed, "Optimal"=df.Wylsacom.optimal, 
                            "Random"=df.Wylsacom.random),  .id = 'baseline')
```

```{r ru_all_setup, include=FALSE}
df.all.ru <- bind_rows(list("AdamThomasMoran"=df.adam,
                            "NTV"=df.ntv,
                            "TheBrianMaps"=df.TheBrianMaps,
                            "ThisIsHorosho"=df.ThisIsHorosho,
                            "Wylsacom"=df.Wylsacom), .id = 'channel')

# Calculate squared sentence length to match Futrell et al.: better predictor than raw length
df.all.ru$sent_len_sq   <- df.all.ru$total_length * df.all.ru$total_length

# Create index variables: ri = 1 if random; 0 else, mi = 1 if optimal; 0 else
df.all.ru$baseline <- factor(df.all.ru$baseline, levels=c("Random", "Optimal", "Observed")) 
df.all.ru <- df.all.ru %>% mutate(ri = as.integer(baseline == "Random"), mi = as.integer(baseline == "Optimal")) %>% filter(dep_length > 5 & dep_length < 300)

# Observed deps for density hexagons
df.all.ru.observed <- df.all.ru %>% filter(baseline=="Observed")
  
# Average deps for fitted scatter plots
df.all.ru.avg <- df.all.ru %>% 
  group_by(channel, baseline, total_length) %>% summarize("avg_length"=mean(dep_length))
df.all.ru.observed.avg <- df.all.ru %>% filter(baseline=="Observed")
```

\pagestyle{fancy}
\fancyhf{}
\lhead{Alex Kramer}
\chead{LING 792 Write-up}
\rhead{Dec. 2019}
\cfoot{\thepage}

\renewcommand{\headrulewidth}{0.2pt}

\renewcommand*{\postnotedelim}{\addcolon}
\DeclareFieldFormat{postnote}{#1}
\DeclareFieldFormat{multipostnote}{#1}

\onehalfspacing

\setlength{\droptitle}{-2em}

\begin{center}
	{\huge Generating Cross-Linguistic Spoken Dependency Corpora via YouTube Subtitles}
\end{center}

<!-- End Header -->
<!---------------->
<!---------------->

\section{Background}

\subsection{Continuous versus discrete variables in typology}

An accurate understanding of cross-linguistic variation and its sources requires an accurate typology. However, large-scale typological databases such as WALS \autocite{dryer2013world} and AUTOTYP \autocite{bickel2017autotyp} classify the world's languages using categorical, or \emph{type-based} \autocite[533--534]{levshinaTokenbasedTypologyWord2019a}, variables, which are necessarily reductive. This reduction biases such typologies toward including only those features that lend themselves to discrete analysis, and further, toward features with a bimodal distribution \autocite[77]{walchliDataReductionTypology2009}. As a result, languages whose features fall at the poles of these distributions are well-classified, while those in-between are not \autocite[91]{walchliDataReductionTypology2009}. Similarly, the classifications given to continuous features will only be accurate to the extent that they happen to fit a bimodal distribution \autocite[78, 91]{walchliDataReductionTypology2009}.

The data reduction caused by the use of categorical features can be mitigated by considering continuous measures where they are more accurate than, or complementary to, categorical variables. Such measures can be obtained through \emph{token-based typology}, which considers "the tokens of linguistic unites or structures observed in language use, as approximated by corpora", as well as "aggregate variables derived from the distributions of usage tokens, such as entropy, complexity, average dependency length, etc" \autocite[534]{levshinaTokenbasedTypologyWord2019a}. These measures reflect language-internal variation with respect to the particular corpus being analyzed and allow languages to be compared along a continuum, rather than placing them into discrete categories that may conflate different sources of variation and different underlying patterns.


\subsection{Word order typology}

Token-based typology has been of growing interest, driven by the increasing availability of both manually-tagged corpus data and the on-going development of automatic machine-learning driven POS taggers and dependency parsers, such as SpaCy (https://spacy.io/), StanfordNLP \autocite{qi2018universal}, and UDPipe \autocite{udpipe2017}. Through corpora, researchers have been able to examine the accuracy of discrete typologies with respect to word order and number marking \autocite{walchliDataReductionTypology2009}, to quantify head direction \autocite{liuDependencyDirectionMeans2010}, to guage the extent of dependency length minimization in natural language \autocite{futrellLargescaleEvidenceDependency2015} and to measure the word order entropy of various dependency relations \autocite{futrellQuantifyingWordOrder2015, levshinaTokenbasedTypologyWord2019a}.

A common thread betwen the above studies is their focus on word order. Word order has traditionally been treated as discrete; WALS and AUTOTYP both categorize languages with respect to "dominant" word order, which can be specified as one of the six logical word orders (SVO, OVS, SOV, OSV, VSO, VOS), as verb position alone (Vxx, xxV, xVx) or as "no dominant order (WALS)"/"free" (AUTOTYP). WALS determines word order from corpora when possible, defining "dominant word order" as the order that occurs at least twice as frequently as the next-most-frequent order; otherwise, grammars are consulted. AUTOTYP employs data only from grammars. Importantly, grammars are themselves a source of data reduction \autocite[77--78]{walchliDataReductionTypology2009}; one thus might expect WALS to report dominant word order more accurately than AUTOTYP for languages for which corpora are available.

AUTOTYP additionally classifies languages as "free", "flexible", or "rigid". However, flexibility here refers only to \emph{structural} flexibility, such that a language like German, which is V-2, will be classified as flexible, while a language like Korean, which is typically SOV but allows for all orders given an appropriate discourse conext \autocite{namboodiripadEnglishdominantKoreanspeakersShow2017}, will be classified as "rigid". Furthermore, as \citet{walchliDataReductionTypology2009} notes, the languages that are classified by WALS as having "no dominant order" do not form a coherent group (88--89); the languages classified as "flexible" or "free" by AUTOTYP are similarly heterogeneous.

Using continuous, corpus-based measures to quantify word order variation not only helps to avoid the use of uninformative categories, but also to capture word order variation that arises from different sources (e.g. discourse-mediated orders versus structurally-required orders). At the same time, they risk conflating these different sources; thus, it may be that word order is best understood by considering discrete and continuous measures \emph{together}, and that both discrete and continuous variable can serve as useful predictors when examining the factors that affect cross-linguistic word order variation and language-internal word order flexibility. 

Corpus-based typological work is not without issues. Drawbacks to the above-cited corpus studies include (1) the modality of the corpus data and (2) comparability across corpora. The corpora employed in these studies are predominantly written, and in the case of \citet{walchliDataReductionTypology2009}, they are all translations of the same original text. A priori, there is no reason to expect written and spoken corpora, or even different types of written or spoken corpora, to reveal the same paterns, as they are subject to different constraints. Indeed, \citet{biberUsingRegisterDiversifiedCorpora1993} finds that corpora containing different modalities and registers make greater or lesser use of structures such as relative clauses, which would affect measures such as dependency length.

Furthermore, although many of the corpora in these studies utilize the Universal Dependencies tagging format, other formats are used, as well, which may have consequences for these measures. (For example, whether function words are treated as heads or dependents will shorten or lengthen dependencies, respectively.) It is not always clear whether head-dependent relationships have been normalized across the corpora used in these studies, thus calling their cross-linguistic comparisons into question.

This project aims to tackle both the lack of spoken data and the issue of cross-linguistic comparability by developing a framework for constructing corpora of informal spoken language through YouTube subtitles. Numerous online tools and code libraries have been developed to allow for the easy downloading of YouTube subtitles, making them a fairly accessible source of data. Subtitles can then be passed through a dependency parsing pipeline (here, StanfordNLP) that uses a standardized tagging format, ensuring that different languages are tagged in a comparable manner.

In developing and testing this framework, I have focused on Japanese and Russian, both of which allow for discourse-mediated flexibility but are typically classified as rigid SOV and rigid SVO, respecively. To get an intial sense of how this data differs from written language and more formal speech, I consider dependency length minimization using the same method as \citet{futrellLargescaleEvidenceDependency2015}. This study found that more
head-final languages like Japanese, Korean, and Turkish showed less minimization than more
head-initial languages, such as Russian; the Japanese corpus, notably, consisted of transcribed speech, but this speech was collected in a formal setting. I hypothesize that this pattern will not hold in informal spoken data, which allows for more word order flexibility and greater use of depency-length-reducing phenomena such as argument-drop than do written sources.



\section{Methods}

\subsection{Building the subtitle corpus}

The top YouTube channels for each language were manually determined on the basis of total subscribers. Of these channels, the top five channels that featured a significant amount of dialogue were selected for analysis. A list of videos for each channel was generated using Selenium to load each channel's "Videos" page and scrape the videos' URLS.

\vspace{1.5em}\begin{minted}[linenos,tabsize=2,breaklines]{python}
def get_links(driver, url, cutoff):

    driver.get(url)

    try:
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, 'items')))
    finally:
        count = 0
        while count < cutoff and scroll_channel(driver, 5):
            count += 1

        elements = driver.find_elements_by_xpath('//*[@id="video-title"]')
        return [(element.get_attribute('href'), element.get_attribute('aria-label'))  for element in elements]
\end{minted}
\vspace{1.5em}

Channels were scrolled either until there were no more videos to load or until a user-defined cutoff was reached. In the most recent iteration of the data, I set the cutoff to 50 scrolls; this resulted in a max of about 1500 URLs per channel.

\vspace{1.5em}\begin{minted}[linenos,tabsize=2,breaklines]{python}
def scroll_channel(driver, pause_time):
    last_height = driver.execute_script('return document.querySelector("#page-manager").scrollHeight')
    driver.execute_script('window.scrollTo(0, document.querySelector("#page-manager").scrollHeight);')
    
    sleep(pause_time)
    
    new_height = driver.execute_script('return document.querySelector("#page-manager").scrollHeight')
    if new_height == last_height:
        return 0
    return 1
\end{minted}
\vspace{1.5em}


Somewhat unexpectedly, Japanese videos were more likely to be subtitled than Russian videos, and the top Japanese channels had more videos overall than the top Russian channels (Tables \ref{sub_totals} and \ref{lang_totals}). Japanese speech is overrepresented in the data by a ratio of approximately 3:2.

```{r include=TRUE, echo=FALSE}
subtitle_totals <- data.frame(read.table(text="Language, ChannelName, Rank, URLsScraped, SubbedVideos, SubbedTimeSeconds
Japanese, Hajime Shachoo, 1, 1530, 864, 265724
Japanese, HikakinTV, 2, 1528, 250, 109806
Japanese, Fischers, 3, 1523, 676, 272108
Japanese, Yuka Kinoshita, 4, 1516, 813, 329794
Japanese, Tokai On Air, 6, 1468, 164, 97588
Russian, TheBrianMaps, 5, 396, 163, 64625
Russian, AdamThomasMoran, 8, 528, 59, 30828
Russian, Wylsacom, 14, 1527, 107, 66183
Russian, Maryana Ro, 16, 150, 41, 15057
Russian, NTV, 19, 1528, 137, 383920
Russian, This is Horosho, 25, 861, 260, 82227", header=TRUE, sep=","))

channel_totals <- subtitle_totals %>% mutate(Proportion = SubbedVideos/URLsScraped)
language_totals <- subtitle_totals %>% group_by(Language) %>% summarise(URLsScraped=sum(URLsScraped), SubbedVideos = sum(SubbedVideos), SubbedTimeHours = sum(SubbedTimeSeconds)/60/60, Proportion = SubbedVideos/URLsScraped)

channel_totals %>%
  kable(format = "latex",
        booktabs = T,
        label = "sub_totals",
        col.names = c("Language", "Channel", "Rank", "URLs Scraped", "Subbed Videos", "Subbed Time (seconds)", "Proportion"),
        caption = "Total subtitled videos by channel",
        ) %>% kable_styling(latex_options = c("hold_position"))

language_totals %>%
  kable(format = "latex",
        booktabs = T,
        label = "lang_totals",
        col.names = c("Language", "URLs Scraped", "Subbed Videos", "Subbed Time (hours)", "Proportion"),
        caption = "Total subtitled videos by language",
        ) %>% kable_styling(latex_options = c("hold_position")) %>% collapse_rows(columns = 1, valign = "middle")
```

After a list of videos was collected, the URLs were processed using the `pytube` module, which provides access to the videos' captions, including whether these captions are auto-generated or manually produced. This distinction is critical, as many videos now include auto-generated captions, but the auto-generation feature still performs quite poorly. Unhelpfully, YouTube does not differentiate auto-generated and manually-entered subtitles on the basis of language code (that is, requesting subs with the code 'ja' will variably return either manual or auto-generated subtitless if both are available). Many of the available subtitle-downloading tools are insufficient in that they assume the distinction between auto-generated and manually-entered subtitles doesn't matter; `pytube` was chosen because it generates a list of captions that differentiates between these two types. However, `pytube` has its own drawbacks: it is not very stable and is not regularly maintained to be up-to-date with changes to YouTube. As such, loading videos or their subtitles sometimes fails for no clear reason.

Before refactoring to use `pytube`, I had written my own code to scrape `www.downsub.com`, which is a website that provides access to YouTube subtitles through its own interface. However, this website has recently updated to use JavaScript to load all pages, and there is no straightforward way to determine the download URLs that point to a given video's subtitles, which are encrypted, thus forcing the switch. I may in the future switch to using the YouTube API (which comes with its own issues) in order to have more direct conrol over the stability of the code.

\vspace{1.5em}\begin{minted}[linenos,tabsize=2,breaklines]{python}
def get_subs(channel, id, url, language):

    try:
        video = YouTube(url)
    except KeyError as e:
        logging.warning("Video {0}: Could not retrieve URL ({1})".format(id, url))
        return 0
    except exceptions.VideoUnavailable as e:
        logging.warning("Video {0}: Video unavailable ({1})".format(id, url))
        return 0
    except:
        logging.critical("Video {0}: An unexpected error occured ({1})".format(id, url))
        return 0

    caption_dict = {caption.name: caption for caption in  video.captions.all()}

    if language in caption_dict.keys():
        write_subs(channel, video, id, url, caption_dict[language])
        logging.info("Video {0}: Manual captions found for (URL: {1} Title: {2})".format(id, url, video.title))
        return int(video.player_config_args['player_response']['videoDetails']['lengthSeconds'])
    else:
        logging.info("Video {0}: No manual captions found (URL: {1} Title: {2})".format(id, url, video.title))
        return 0
\end{minted}
\vspace{1.5em}


\subsection{Parsing the subtitles}

Downloaded subtitles were pre-processed to remove symbols, emojis, and other features of informal captions that cause trouble for StanfordNLP, such as unusual punctuation or lack of punctuation. Additional processing is done to remove speaker attributions and parentheticals. Unfortunately, this pre-processing is an initial source of error in the data. In particular, subtitlers do not all follow the same conventions in delineating utterances: some write one complete senence per line; others break up sentences across lines. Some use punctuation, but most do not. As an initial heuristic, I have assumed that each line is one sentence and added punction where appropriate to aid the parser, which assumes that all sentences end with periods. However, this heuristic will artificially deflate dependency lengths whenever an individual line contians less than a full sentence and fails to indicate this with a comma.

\vspace{1.5em}\begin{minted}[linenos,tabsize=2,breaklines]{python}
def preprocess_subtitles_ja(subtitles):
    for line in subtitles:
        if line and not re.search("^[0-9]", line):

                line = emoji.remove_emoji(line.strip())

                line = re.sub(r'（[^)]*）', '', line) # Remove Japanese parens
                line = re.sub(r'\([^)]*\)', '', line) # Remove English parens
                line = re.sub(r'<[^)]*>',   '', line) # Remove HTML decorations
                line = re.sub(r'（[^)]*\)', '', line) # Sometimes the wrong close paren is used
            
                close_paren = line.find("）")
                open_paren = line.find("（")
                
                # Unmatched close paren is usually speaker attribution
                # Need to remove this so it's not parsed as something strange
                if close_paren > -1: 
                    line = line[close_paren+1:]
                    
                # Unmatched open paren is usually followed by sound effects or jibberish
                if open_paren > -1: 
                    line = line[:open_paren]

                # Remove symbols: can probably condense this
                line = re.sub("[♫♡♥♪→↑↖↓←”wｗW⇓〜\~【】《》「」\[\]\n]", "", line)
                
                # Remove odd punctuation: need something more comprehensive?
                line = re.sub("[　！‼？!?.…]", "。", line)

                if(line):
                    if(line[-1] != '。' and line[-1] != '、'):
                        line += '。'
                        
                    # Sometimes speakers are delimited with : instead
                    line_no_speaker = re.split("[：:]", line)
                    if len(line_no_speaker) > 1:
                        yield ("".join(no_speaker[1:]))
                    else:
                        yield line
\end{minted}
\vspace{1.5em}

Russian subtitles were less likely to contain extraneous characters, but similar to Japanese subtitles, it was rare that sentences were demarcated by periods. Thus, the Russian subtitles were subject to less heavy processing overall, but nonetheless suffer from the same issue of deflated dependency lengths.

\vspace{1.5em}\begin{minted}[linenos,tabsize=2,breaklines]{python}
def preprocess_subtitles_ru(subtitles):
    for line in subtitles:
        if line and not re.search("^[0-9]", line):

            line = remove_emoji(line.strip())
            line = line.replace(":D", "")
            line = line.replace(":)", "")

            line = re.sub(r'\([^)]*\)', '', line) # Remove parens
            line = re.sub(r'<[^)]*>', '', line)   # Remove HTML
            line = re.sub("[♫♡♥♪→↑↖↓←⇓\(\)\[\]\n]", "", line)
            line = re.sub("[!?]", ".", line)

            if(line):
                if(line[-1] != '.' and line[-1] != ','):
                    line += '.'
                no_attr = re.split("[:]", line)
                if len(no_attr) > 1:
                    yield ("".join(no_attr[1:]))
                else:
                    yield line
\end{minted}
\vspace{1.5em}

After preprocessing, the subtitles were parsed using the StanfordNLP pipeline. This step inroduces another layer of error/variability: Japanese text is more diffcult to parse accurately than languages like Russian due to, for example, its lack of spaces and the optionality of case marking in informal speech, which in other languages facilitate word and part-of-speech recognition. Furthermore, the parser is not trained to recognize non-canonical orders beyond OSV, and can only recognize OSV reliably when case marking is present. Verb-medial and verb-initial orders are mis-parsed as relative clauses and adjuncts, which inflates dependency lengths.

\vspace{1.5em}\begin{minted}[linenos,tabsize=2,breaklines]{python}
def process_files(channel, language, subtitles_fns, nlp):

    dep_path = join("dependencies", language, channel)
    if not exists(dep_path):
        makedirs(dep_path)

    observed_fn = join(dep_path, channel + "_observed_dependencies.csv")
    optimal_fn  = join(dep_path, channel + "_optimal_dependencies.csv")
    random_fn   = join(dep_path, channel + "_random_dependencies.csv")

    with open(observed_fn, "w") as observed_out, \
         open(optimal_fn,  'w') as optimal_out,  \
         open(random_fn,   'w') as random_out:

        out_files = (observed_out, optimal_out, random_out)
        header = "video_id, sentence_id, dep_length, total_length\n"

        for f in out_files:
            f.write(header)

        video_id = 0
        for subtitles_fn in subtitles_fns:
            with open(subtitles_fn, "r") as subtitles_in:

                video_id += 1
                preprocessed_subtitles = list(preprocess_subtitles_ja(subtitles_in))

                if len(preprocessed_subtitles) != 0:
                    nlp_subtitles = nlp("。".join(preprocessed_subtitles))
                    process_dependencies(video_id, nlp_subtitles, observed_out, optimal_out, random_out)
\end{minted}
\vspace{1.5em}

Finally, the total dependency length of each parsed sentence was calculated. Following \citet{futrellLargescaleEvidenceDependency2015}, this measure is simply the sum of the lengths of all dependencies in the sentence. Ultimately, only sentences with more than 5 dependencies and fewer than 30 dependencies were considered. This served to both reduce the amount of data being generated and limit conclusions to sentence lengths for which a reasonable number of observations was available.

\vspace{1.5em}\begin{minted}[linenos,tabsize=2,breaklines]{python}
def process_dependencies(video_id, doc, observed_out, optimal_out, random_out):
    sent_id = 0
    for sentence in doc.sentences:
        sent_id += 1

        true_dependencies = [dependency for dependency in sentence.dependencies if dependency[1] not in ["punct"]]
        num_dependencies = len(true_dependencies)

        if(num_dependencies < 5 or num_dependencies > 40):
            continue

        try:
            dependency_tree = tree(true_dependencies)
        except:
            logging.warning("Tree construction failed for sentence {0}".format(sent_id))
            continue

        optimal_dependencies = list(iter_flatten(linearize_optimal(dependency_tree[0])))
        dependencies = list(zip(true_dependencies, optimal_dependencies))

        true_indices, optimal_indices = ({}, {})
        for i in range (0, len(dependencies)):
            true_indices.update({true_dependencies[i][2].index: i + 1})
            optimal_indices.update({optimal_dependencies[i][2].index: i + 1})

        dep_total_true, dep_total_optimal = (0, 0)

        for (true_dep, optimal_dep) in dependencies:
            dep_total_true += get_dependency_length(true_dep, true_indices)
            dep_total_optimal += get_dependency_length(optimal_dep, optimal_indices)

        process_random(sentence, video_id, sent_id, num_dependencies, dependency_tree, random_out)

        if(dep_total_optimal > dep_total_true):
            count_bad += 1
        if num_dependencies > 5:
            logging.info("Video: {2}, Sentence: {3}, Observed: {0}, Optimal: {1}".format(dep_total_true, dep_total_optimal, video_id, sent_id))

        observed_out.write("{0}, {1}, {2}, {3}\n".format(video_id, sent_id, dep_total_true, num_dependencies))
        optimal_out.write("{0}, {1}, {2}, {3}\n".format(video_id, sent_id, dep_total_optimal, num_dependencies))

def iter_flatten(iterable):
  it = iter(iterable)
  for e in it:
    if isinstance(e, list):
      for f in iter_flatten(e):
        yield f
    else:
      yield e

def tree(dependencies):
    nodes={}
    for i in dependencies:
        parent, rel, child = i
        nodes[child] = {"parent": parent, "child": child, "relation": rel, "children": []}

    forest = []
    for i in dependencies:
        parent, rel, child = i
        node = nodes[child]

        if rel == 'root': # this should be the Root Node
                forest.append(node)
        else:
            parent = nodes[parent]
            children = parent['children']
            children.append(node)
    return forest
    
\end{minted}
\vspace{1.5em}

In addition to the actual (i.e., observed) total dependency length, a random baseline and optimal baseline were calculated. The random baseline was calculated as follows: For each head, the head and its dependents were ordered randomly. This process was then repeated with each dependent as a head until no dependents remained.

For each sentence, only one random baseline was calculated, as opposed to the 100 calculated in \citet{futrellLargescaleEvidenceDependency2015}, to reduce the volume of data being generated. Subsets of the data for which 10, 25, and 100 random baselines were calculated per sentence all showed a similar pattern.

\vspace{1.5em}\begin{minted}[linenos,tabsize=2,breaklines]{python}
def linearize_random(node):

    if not len(node['children']):
        return [(node['parent'], node['relation'], node['child'])]

    else:
        chunk = []
        for child in node['children']: # Randomize each child and append it
            chunk.append(linearize_random(child))
        chunk.append((node['parent'], node['relation'], node['child']))
        random.shuffle(chunk)

        return chunk
        
def process_random(sentence, video_id, sent_id, num_dependencies, dependency_tree, random_out):
    for i in range(0, 10):

        dep_total_random = 0
        random_indices = {}

        random_dependencies =  list(iter_flatten(linearize_random(dependency_tree[0])))
        for j in range (0, len(random_dependencies)):
            random_indices.update({random_dependencies[j][2].index: j + 1})

        for random_dep in random_dependencies:
            dep_total_random += get_dependency_length(random_dep, random_indices)

        random_out.write("{0}, {1}, {2}, {3}\n".format(video_id, sent_id, dep_total_random, num_dependencies))
\end{minted}
\vspace{1.5em}

Optimal baselines were generated in the following manner \autocite{gildeaOptimizingGrammarsMinimum}: For each head, order its dependents by weight, where weight is defined as the total number of children contained under the dependent. Place the dependents on alternating sides of the head in order of weight, such that the heaviest dependents are the farthest from the head. The link between the head and its parent node is treated as a special child that is placed opposite the head's longest real child.

\vspace{1.5em}\begin{minted}[linenos,tabsize=2,breaklines]{python}
def linearize_optimal(node, right=True):

    if not len(node['children']):
        return [(node['parent'], node['relation'], node['child'])]

    else:

        sorted_children = node['children']
        sorted_children.sort(key=weight, reverse=True)
        chunk = [(node['parent'], node['relation'], node['child'])]

        root_pos = 0
        for i in range(0, len(sorted_children)):
            weight_cur = weight(sorted_children[i])
            if (i % 2 and right) or (not i % 2 and not right): # Add the largest child to the right of the parent, then swap directions
                chunk.insert(root_pos + 1, linearize_optimal(sorted_children[i], False))
            else: # Add largest to left and swap
                chunk.insert(root_pos, linearize_optimal(sorted_children[i], True))
                root_pos = root_pos + 1

        return chunk
        
def weight(node):
    if not len(node['children']):
        return 1
    return 1 + sum(map(weight, node['children']))
\end{minted}
\vspace{1.5em}


\section{Results}

\subsection{Japanese}

As predicted, total dependency lengths were closer to the optimal baseline in the present corpus than they were in the corpus used by \citet{futrellLargescaleEvidenceDependency2015}. Figure \ref{avg_all_jp} shows the average total dependency length by sentence length aggregated across channels. Random, optimal, and observed average total dependency lengths were each fitted with a generalized additive model to visualize their trends; density hexagons show the number of observations at different lengths.

```{r avg_all_jp, include=TRUE, echo=FALSE, fig.cap="Average total dependency lengths for sentences in Japanese with total length > 5 and < 300", fig.align='left', fig.show='hold'}
all.dependencies.jp.g <- ggplot(df.all.jp.avg, aes(y=avg_length, x=total_length, color=baseline, group=baseline)) + 
    stat_bin_hex(data=df.all.jp.observed.avg, bins=20, show.legend=FALSE, inherit.aes=FALSE, 
                 aes(y=dep_length, x=total_length, fill=..density..)) +
    scale_fill_gradientn(colours=c("white","black")) +
    geom_smooth(aes(group=baseline), se = TRUE, method = "gam", formula = y ~ s(log(x))) + 
    scale_color_manual(values = ghibli_palette("PonyoMedium")[c(3,4,6)], labels=c("Random", "Optimal", "Observed")) + 
    labs(title="", x="Sentence Length", y="Total Dependency Length", color="") + 
    theme(legend.position="right")

all.dependencies.jp.g
```

Breaking down the aggregate results by channel does not seem to reveal any strong difference between channels upon visual inspection (Figure \ref{avg_channel_jp}), although preliminary results had suggested that the average total dependency lengths for the channel YukaKinoshina might be slightly higher than those of the other channels. Yuka's speech is overall more formal than that of the other YouTubers included in this sample, so it could have been expected that her speech would employ longer dependencies; this doesn't seem to be borne out, although I have not yet been able to analyze the data statistically. It is possible that spoken sources that are more differentiated in terms of register from those used here, for example news programs, would pattern differently.

```{r avg_channel_jp, include=TRUE, echo=FALSE, fig.cap="Average total dependency lengths by channel in Japanese with total length > 5 and < 300", fig.align='left', fig.show='hold'}
# Plot by channel
dep.by.channel.jp.g <- ggplot(df.all.jp.avg, aes(y=avg_length, x=total_length, color=baseline, group=channel)) + 
    stat_bin_hex(data=df.all.jp.observed, bins=20, show.legend=FALSE, inherit.aes=FALSE, 
                 aes(y=dep_length, x=total_length, fill=..density..)) +
    scale_fill_gradientn(colours=c("white", "black")) +
    geom_smooth(aes(group=baseline), se = FALSE, method = "gam", formula = y ~ s(log(x))) + 
    labs(title="", x="Sentence Length", y="Total Dependency Length", color="") + 
    scale_color_manual(values = ghibli_palette("PonyoMedium")[c(3,4,6)], labels=c("Random", "Optimal", "Observed")) +
    theme(legend.position="right") +
    facet_wrap(.~channel)

dep.by.channel.jp.g
```

In addition to dependency lengths, I was also interested in examining word order flexibility. However, various issues with StanfordNLP's handling of the text made this infeasible. In particular, to my knowledge, all Japanese parsers have been trained on written data, which differs from informal speech in ways that facilitate the parser. In particular, case marking and verb-final order are near-obligatory; thus, as mentioned in \S\ref{method}, parsers are good at identifying SOV and OSV order, but they assume that verbs are always final. Compounding this issue is the fact that relative clauses are not always morphologically distinguished from verb-medial and verb-initial orders. This means that the main-clause verbs of sentences in non-canonical orders are reliably parsed as dependents of whatever noun follows them, rather than as roots.

Nonetheless, manual analysis of a small subset of the collected subtitles found that non-verb-final orders are present in the data, though infrequent. Future work could aim to quantify the frequency of such sentences, for example by developing a heuristic to collect likely mis-parses and manually analyzing them, or by hand-correcting a subset of the data and retraining a parser on this corrected data.

\subsection{Russian}

The Russian subtitles, contra the hypothesis that spoken language would contain shorter dependencies, appear to pattern in roughly the same way here as they did in \citet{futrellLargescaleEvidenceDependency2015}, and their dependency lengths appear grow at a faster rate than those of the Japanese subtitles (Figure \ref{ru_all_avg}). This pattern holds in each individual channel, as well (Figure \ref{avg_channel_ru}).Though this result was initially surprising, it is perhaps not so unexpected; \citet{futrellLargescaleEvidenceDependency2015} hypothesize that languages with richer morphology may have more freedom in their dependency lengths because this morphology makes long dependencies easier to identify; Russian, which possesses rich inflectional morphology, may then tolerate longer dependencies than Japanese, which in informal registers allows case markers to be omitted. Furthermore, Russian typically does not allow pro- or argument-drop, whereas spoken Japanese sounds unnatural \emph{without} extensive argument-dropping. This, too, would lead to shorter dependencies in spoken Japanese.

```{r ru_all_avg, include=TRUE, echo=FALSE, fig.cap="Average total dependency lengths for sentences with total length > 5 and < 300", fig.align='left', fig.show='hold'}
all.dependencies.ru.g <- ggplot(df.all.ru.avg, aes(y=avg_length, x=total_length, color=baseline, group=baseline)) + 
    stat_bin_hex(data=df.all.ru.observed.avg, bins=20, show.legend=FALSE, inherit.aes=FALSE, 
                 aes(y=dep_length, x=total_length, fill=..density..)) +
    scale_fill_gradientn(colours=c("white", "black")) +
    geom_smooth(aes(group=baseline), se = TRUE, method = "gam", formula = y ~ s(log(x))) + 
    scale_color_manual(values = ghibli_palette("PonyoMedium")[c(3,4,6)], labels=c("Random", "Optimal", "Observed")) + 
    labs(title="", x="Sentence Length", y="Total Dependency Length", color="") + 
    theme(legend.position="right")

all.dependencies.ru.g
```

```{r avg_channel_ru, include=TRUE, echo=FALSE, fig.cap="Average total dependency lengths by channel in Russian with total length > 5 and < 300", fig.align='left', fig.show='hold'}
# Plot by channel
dep.by.channel.ru.g <- ggplot(df.all.ru.avg, aes(y=avg_length, x=total_length, color=baseline, group=channel)) + 
    stat_bin_hex(data=df.all.ru.observed, bins=20, show.legend=FALSE, inherit.aes=FALSE, 
                 aes(y=dep_length, x=total_length, fill=..density..)) +
    scale_fill_gradientn(colours=c("white", "black")) +
    geom_smooth(aes(group=baseline), se = FALSE, method = "gam", formula = y ~ s(log(x))) + 
    labs(title="", x="Sentence Length", y="Total Dependency Length", color="") + 
    scale_color_manual(values = ghibli_palette("PonyoMedium")[c(3,4,6)], labels=c("Random", "Optimal", "Observed")) +
    theme(legend.position="right") +
    facet_wrap(.~channel)

dep.by.channel.ru.g
```

However, it could also be that factors external to the languages are responsible for the observed patterns. Although I tried to select Russian channels that were similar in tone and content to the Japanese channels, the top Russian channels on YouTube tend to be more expository, while the Japanese channels are more action- and interaction-focused. In addition, my code has been tested thoroughly only in the case of Japanese, which was the focus of its initial development. Unexpected issues arose in the processing of the Russian data; for example, StanfordNLP was much slower to process it and occasionally ran into recursion errors when processing one of the channels, and when handling a different channel, my own code returned odd results, such as the "optimal" dependency lengths of a handful of sentences being longer than the observed lengths. Thus, if the Japanese results already must be taken with a grain of salt, this goes double for Russian.

\section{Conclusion}

This project aimed to develop a preliminary framework for developing dependency corpora from YouTube subtitles. The results are promising: it is relatively easy to acquire the subtitles for a large number of videos, and the subtitles are overall of good quality. Issues arise primarily in the preprocessing and parsing of the subtitles, especially in the case Japanese, which, especially when it comes to spoken data, still lacks reliable automatic parsing. Depending on the language, it is also more or less difficult to find a sufficient amount of subtitles; a surprising outcome of this work was the relative lack of Russian subtitles when compared to Japanese.

A basic analysis of the total dependency lengths of the sentences collected found that informal spoken Japanese contained shorter dependencies than those from spoken Japanese collected in a more formal setting, pointing to a potential effect of register. However, dependency lengths in informal spoken Russian did not differ appreciably from those calculated from a written corpus. Without a more refined analysis, though, it is difficult to determine the source of this decrepancy.

A straightforward way to begin improving the corpus would be to diversify the channels included in the Japanese sample and to balance out the Japanese and Russian data. Tackling the insufficiency of the Japanese parser would also be an ideal, though more involved, next step. There are also a number of issues to work out before it can be certain that the Russian parser is behaving as intended. Still, as both the code and the languge models it depends on improve, this data can only become more useful, and I believe it has the potential to serve as an important complement to the written data that is currently over-represented in token-based typology.

\printbibliography{}