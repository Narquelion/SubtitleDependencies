---
title: "YouDePP/UD - By-sentence features"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
---

# Processing

## Set-up

```{r setup, include=F, echo=F, eval=T}

knitr::opts_chunk$set(echo = TRUE)

library(dplyr)

# Package names
packages <- c("ggplot2", "ggeffects", "ghibli", "lavaan", "psych", "lme4", "lmerTest", "tidyr", "GPArotation", "ggpubr")

# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

# Packages loading
lapply(packages, library, character.only = TRUE) %>%
  invisible()

Sys.setlocale(category = "LC_CTYPE", locale = "C")

```

### Functions

```{r utilities, include=F, echo=F, eval=T}

# Is it better to sample with replacement, or to only take *up to* x * prop samples per length?
sample_corpus <- function(df, num_samples, transitive_only=T) {
  
  # Sample x * prop observed sentences per sentence length and language
  df.sample.observed <- df %>% 
    filter(!grepl("Auto", corpus) & baseline=="observed" & (is_transitive==1 | transitive_only==F)) %>%
    mutate(rand_pos=0) %>% 
    group_by(language, corpus) %>% mutate(num_sentences=n()) %>%
    group_by(language, corpus, numDeps_noFunc) %>% mutate(prop_length=n()/num_sentences) %>% 
    sample_n(min(round(num_samples * prop_length), n()), replace=F) %>% ungroup() %>% 
    group_by(id) %>% mutate(sample_id=1:n(), reps=0, rand_pos=-1) %>% ungroup() %>%
    select(-num_sentences, -prop_length)
  
  # Get the number of times each id is repeated
  df.id_counts <- df.sample.observed %>% group_by(id) %>% summarize(reps=n())
  
  # Filer the random baseline for our selected ids and rep the random sentences x times
  df.sample.random <- df %>%
    filter(baseline=="random" & id %in% df.id_counts$id) %>% # Use only sentences in our observed sample!
    group_by(baseline, id) %>% mutate(rand_pos=ifelse(baseline=="random", 1:n(), 0)) %>% ungroup() %>%
    merge(df.id_counts) %>% group_by(id) %>%
    slice(rep(1:n(), each = head(reps, 1))) %>%
    ungroup() %>%
    mutate(sample_id=1:n(), sample_id=sample_id%%reps+1) %>% ungroup()
  
  # Do the same for the optimal data
  df.sample.optimal <- df %>%
    filter(baseline=="optimal" & id %in% df.id_counts$id) %>%
    merge(df.id_counts) %>% group_by(id) %>%
    slice(rep(1:n(), each = head(reps, 1))) %>%
    ungroup() %>% mutate(rand_pos=0) %>% 
    group_by(id) %>% mutate(sample_id=1:n(), rand_pos=-1) %>% ungroup()
  
  # Re-combine the baselines
  return(bind_rows(list(df.sample.observed, df.sample.random, df.sample.optimal)) %>% select(-sample_id, -reps))
}

# Calculate entropy as well as average DL and headedness values per sentence length

# 1. Take the average of the dependency length measurements
# 2. Calculate the proportions of different orders
# 3. Calculate the entropy of transitive sentences
# 4. Calculate mean head finality

average_diff <- function(df) {
  return(df %>% group_by(language, corpus, num_deps) %>% summarize(
    
     totalDLDiff=mean(totalDLDiff),          # Average dependency data
     averageDL=mean(averageDLDiff), 
     averageDLSLDiff=mean(averageDLSLDiff),
     
     transitive=sum(is_transitive)/n(),     # Should be all sentences in this case
     
     sov=sum(is_sov)/sum(is_transitive),    # Calculate proportion of each order; should sum to 1
     osv=sum(is_osv)/sum(is_transitive), 
     vso=sum(is_vso)/sum(is_transitive), 
     vos=sum(is_vos)/sum(is_transitive), 
     svo=sum(is_svo)/sum(is_transitive), 
     ovs=sum(is_ovs)/sum(is_transitive),
     
     entropyTrans=(-1 * ((sov * log2(max(sov, 0.000000001))) +  # The 0.00001 is a placeholder
                       (osv * log2(max(osv, 0.000000001))) + 
                       (vso * log2(max(vso, 0.000000001))) + 
                       (vos * log2(max(vos, 0.000000001))) + 
                       (svo * log2(max(svo, 0.000000001))) + 
                       (ovs * log2(max(ovs, 0.000000001))))),
  
     head_finality=mean(head_finality)) %>% ungroup() # Average headedness
  )
}
  
gg_theme <- theme_bw() + 
  theme(
    text = element_text(size = 25),
    #axis.text=element_text(size=25), 
    plot.title = element_text(size=25, face="bold"),
    legend.text = element_text(size = 20),
    legend.title = element_text(size = 20),
    legend.position = "bottom",
    axis.ticks = element_line(colour = "grey70", size = 0.2),
    panel.grid.major = element_line(colour = "grey70", size = 0.2),
    panel.grid.minor = element_blank()
  )

```

```{r load_data, include=F, echo=F, eval=T}

Sys.setlocale(category = "LC_CTYPE", locale = "C")
load(file='~/Documents/research/git-projects/YouDePP/data/rdata/youdepp_stats_clean.Rda')
load(file='~/Documents/research/git-projects/YouDePP/data/rdata/ud_stats_clean.Rda')

df.youdepp.stats <- df.youdepp.stats %>% 
  filter(is_transitive == 1) %>% 
  select(-is_intransitive, -is_subjdrop, -is_vonly, -is_sv, -is_vs, -is_ov, -is_vo)
df.ud.stats <- df.ud.stats %>% 
  filter(is_transitive == 1) %>% 
  select(-is_intransitive, -is_subjdrop, -is_vonly, -is_sv, -is_vs, -is_ov, -is_vo)

```

### Randomly sample data (legacy code---no longer doing this)

Transitive (that is, \{S, O, V\}) sentences are randomly sampled without replacement to create a more manageable and balanced data set. The number of samples taken at each sentence length for each corpus/channel for each language is the minimum of *either* 300/6000 (YouDePP/UD) * the proportion of sentences of a given length to the total number of sentences, *or* the total number of sentences of a given length. Sentence lengths are grouped by the number of *open-class dependencies* in the sentence. Automatically-transcribed speech data is excluded. Two different sampling factors were chosen in order to account for differences in the overall distribution of sentences in the UD and YouDePP corpora, as well as to account for the fact that each language in the YouDePP sample is divided across multiple "channels", while there is only one corpus per language in the UD sample. These sampling factors resulted in a fairly balanced number of sentences across the two data sets (YouDePP n=7455, UD n=7289).

```{r sample_data, include=T, echo=F, eval=T, message=F}

# Sample YouDePP and UD corpora
# Using different values corrects for the differences in distribution (?)
df.youdepp.stats.sampled <- sample_corpus(df.youdepp.stats, 250)
df.ud.stats.sampled      <- sample_corpus(df.ud.stats, 6000)

# Density of samples
ggplot(df.youdepp.stats.sampled, aes(x=numDeps_noFunc)) +
  geom_density() + facet_wrap(.~language) + gg_theme
ggplot(df.ud.stats.sampled, aes(x=numDeps_noFunc)) +
  geom_density() + facet_wrap(.~language) + gg_theme

df.youdepp.stats.sampled %>% filter(baseline=="observed") %>% summarize(count=n())
df.ud.stats.sampled %>% filter(baseline=="observed") %>% summarize(count=n())

# Combine datasets
#df.all.stats.sampled <- bind_rows(list("spoken"=df.youdepp.stats.sampled, "written"=df.ud.stats.sampled), .id="modality")

```

### Calculate difference scores

Difference scores are calculated as the observed dependency lengths of the sentence minus the optimal dependency lengths (for all three of: total DL, average DL, and average DL/sentence length), considering open-class words only. The function `difference_score()` only returns values relevant for looking at entropy and headedness:

```{r calculate_difference_scores}

difference_score_noFunc <- function(df) {
  
  df.ran <- df %>% filter(baseline=="random") %>% 
    select(id, totalDL_noFunc, averageDL_noFunc, averageDLSL_noFunc) %>%
    rename(totalDL_noFunc_ran=totalDL_noFunc, averageDL_noFunc_ran=averageDL_noFunc, averageDLSL_noFunc_ran=averageDLSL_noFunc)
  df.obs <- df %>% filter(baseline=="observed")
  
  df.diff <- merge(df.ran, df.obs, by="id")
  
  return(df.diff %>% 
           mutate(totalDLDiff=(totalDL_noFunc_ran-totalDL_noFunc), 
                  averageDLDiff=(averageDL_noFunc_ran-averageDL_noFunc), 
                  averageDLSLDiff=(averageDLSL_noFunc_ran-averageDLSL_noFunc)) %>%
           select(language, corpus, id, numDeps_noFunc, headFinality_noFunc,
                  is_transitive, is_sov, is_osv, is_vso, is_vos, is_svo, is_ovs, 
                  totalDLDiff, averageDLDiff, averageDLSLDiff) %>%
           rename(num_deps=numDeps_noFunc, head_finality=headFinality_noFunc))
}

difference_score_all <- function(df) {
  
  df.ran <- df %>% filter(baseline=="random") %>% 
    select(id, totalDL_all, averageDL_all, averageDLSL_all) %>%
    rename(totalDL_all_ran=totalDL_all, averageDL_all_ran=averageDL_all, averageDLSL_all_ran=averageDLSL_all)
  df.obs <- df %>% filter(baseline=="observed")
  
  df.diff <- merge(df.ran, df.obs, by="id")
  
  return(df.diff %>% 
           mutate(totalDLDiff=(totalDL_all_ran-totalDL_all), 
                  averageDLDiff=(averageDL_all_ran-averageDL_all), 
                  averageDLSLDiff=(averageDLSL_all_ran-averageDLSL_all)) %>%
           select(language, corpus, id, numDeps_all, headFinality_all,
                  is_transitive, is_sov, is_osv, is_vso, is_vos, is_svo, is_ovs, 
                  totalDLDiff, averageDLDiff, averageDLSLDiff)  %>%
           rename(num_deps=numDeps_all, head_finality=headFinality_all))
}

# Sanity check
#df.youdepp.stats.diff <- difference_score_all(df.youdepp.stats)
#df.ud.stats.diff      <- difference_score_all(df.ud.stats)

# NOTE: Currently not actually randomly sampled!
df.youdepp.stats.sampled.diff <- difference_score_noFunc(df.youdepp.stats)
df.ud.stats.sampled.diff      <- difference_score_noFunc(df.ud.stats)

```

### Calculate entropy

```{r setup_entropy, echo=F, include=F, eval=T}

# Get entropy per sentence length and channel (where available) + average values
df.youdepp.stats.sampled.diff.avg <- df.youdepp.stats.sampled.diff %>% average_diff
df.ud.stats.sampled.diff.avg      <- df.ud.stats.sampled.diff      %>% average_diff

# Add entropy to by-sentence data
df.youdepp.stats.sampled.diff.ent <- merge(df.youdepp.stats.sampled.diff, 
                                           df.youdepp.stats.sampled.diff.avg %>% 
                                             select(language, corpus, num_deps, entropyTrans)) %>% 
  filter_all(all_vars(!is.na(.) & !is.infinite(.) & !is.nan(.)))

df.ud.stats.sampled.diff.ent      <- merge(df.ud.stats.sampled.diff, 
                                           df.ud.stats.sampled.diff.avg %>% 
                                             select(language, corpus, num_deps, entropyTrans)) %>% 
  filter_all(all_vars(!is.na(.) & !is.infinite(.) & !is.nan(.)))

```

## Baseline graphs (for vizualization)

```{r setup_nice_graphs}

df.all.stats <- bind_rows(list("Spoken (YouTube)" = df.youdepp.stats, "Written (UD)" = df.ud.stats), .id="modality")

df.all.stats <- df.all.stats %>% mutate(language_clean = case_when(language == "fr" ~ "French", language == "ru" ~ "Russian", language == "ja" ~ "Japanese", language == "it" ~ "Italian", language == "en" ~ "English", language == "ko" ~ "Korean", language == "tr" ~ "Turkish"))

df.all.stats.sov <- df.all.stats %>% filter(language == "ja" | language == "ko" | language == "tr" | language == "ru") # Hack to get heights the same
df.all.stats.svo <- df.all.stats %>% filter(language != "ja" & language != "ko" & language != "tr")

```

```{r dependency_graphs_sov}

gg.sov.dependencies <- ggplot(df.all.stats.sov  %>% filter(numDeps_noFunc <= 10), 
                              aes(x=numDeps_noFunc, y=totalDL_noFunc, color=baseline, group=baseline)) + 
  
    stat_bin_hex(data=df.all.stats.sov %>% filter(baseline=="observed" & numDeps_noFunc <= 10), 
                 bins=50, binwidth=c(1, 10), show.legend=FALSE, inherit.aes=FALSE, 
                 aes(x=numDeps_noFunc, y=totalDL_noFunc, alpha=log(..density..)), fill="#c6d6d6") + 
  
    geom_smooth(aes(group=baseline), se = TRUE, method = "gam", formula = y ~ s(log(x), k=7)) + 
    scale_color_manual(values=c("#61c75d", "#0086d6", "#001C49"), labels=c("Random", "Observed", "Optimal")) +
  
    labs(title="", x="Sentence Length", y="Total Dependency Length", color="Baseline", linetype="Corpus") + 
  
    theme(legend.position="bottom") +
    facet_grid(language_clean ~ modality) +
  
    coord_cartesian(ylim=c(0, 40)) +
  
    gg_theme

ggsave("plots/total_dependencies_sov_noFunc.png", plot=gg.sov.dependencies, device="png", dpi = 300, height=7.25, width=8)
gg.sov.dependencies

```

```{r dependency_graphs_svo}

gg.svo.dependencies <- ggplot(df.all.stats.svo %>% filter(numDeps_noFunc <= 10), aes(x=numDeps_noFunc, y=totalDL_noFunc, color=baseline, group=baseline)) + 
  
    stat_bin_hex(data=df.all.stats.svo %>% filter(baseline=="observed" & numDeps_noFunc <= 10), 
                 bins=50, binwidth=c(1,10), show.legend=FALSE, inherit.aes=FALSE, 
                 aes(x=numDeps_noFunc, y=totalDL_noFunc, alpha=log(..density..)), fill="#c6d6d6") + 
  
    geom_smooth(aes(group=baseline), se = TRUE, method = "gam", formula = y ~ s(log(x), k=7)) + 
    scale_color_manual(values=c("#61c75d", "#0086d6", "#001C49"), labels=c("Random", "Observed", "Optimal")) +
  
    labs(title="", x="Sentence Length", y="Total Dependency Length", color="Baseline", linetype="Corpus") + 
  
    theme(legend.position="bottom") +
    facet_grid(language_clean ~ modality) +
  
    coord_cartesian(ylim=c(0, 40)) +
  
    gg_theme

ggsave("plots/total_dependencies_svo_noFunc.png", plot=gg.svo.dependencies, device="png", dpi = 300, height=9, width=8)
gg.svo.dependencies

```

```{r arrange_baseline_graphs}

gg.total_dependencies.arranged <- ggarrange(gg.svo.dependencies, gg.sov.dependencies, ncol=2, common.legend = TRUE, legend = "bottom") + theme(legend.text = element_text(20), legend.title = element_text(20))

ggsave("plots/total_dependencies_noFunc.png", plot=gg.total_dependencies.arranged, device="png", dpi = 300, height=9, width=16)

gg.total_dependencies.arranged


```


# Difference score graphs

It seems like, in most languages, there's not a huge difference between modalities when we look at this this narrower sample of sentences (i.e., transitive sentences for which headedness could be calculated) and dependency types (open-class only).

```{r graph_setup}

df.all.sampled.diff.ent <- bind_rows(list("spoken"=df.youdepp.stats.sampled.diff.ent, "written"=df.ud.stats.sampled.diff.ent), .id="modality") %>%
  filter_all(all_vars(!is.na(.) & !is.infinite(.) & !is.nan(.))) %>%
  group_by(language, corpus, modality, id) %>%
  summarize(num_deps = mean(num_deps), totalDLDiff = mean(totalDLDiff), head_finality=mean(head_finality), entropyTrans)

```

```{r difference_graphs_youdepp_narrow, eval=TRUE,  echo=FALSE, include=TRUE}

gg.youdepp.dependencies.diff <- ggplot(df.all.sampled.diff.ent, aes(x=num_deps, y=totalDLDiff)) + 
    stat_bin_hex(data=df.all.sampled.diff.ent,
                 inherit.aes=FALSE,  aes(x=num_deps, y=totalDLDiff, alpha=(1/10)*log(..density..)), 
                 bins=50, binwidth = c(1,2),
                 show.legend=FALSE, fill="black") +
    geom_smooth(aes(color=language), se = TRUE, method = "lm", formula = y ~ x) + 
    scale_fill_gradientn(colours=c("#f8f9f5","#000000")) +
    labs(title="", x="Sentence Length", y="Random-Observed", color="") + 
    theme(legend.position="bottom") +
    facet_grid(modality~language) + 
    gg_theme

#ggsave("plots/youdepp_dependencies_diff.png", plot=gg.youdepp.dependencies.diff, device="png", dpi = 300, width=10)
gg.youdepp.dependencies.diff

```

# Headedness

### Overall distributions

```{r graphs_overall_headedness}

gg.violin.overall.headedness <- ggplot(df.all.sampled.diff.ent, aes(x=language, y=head_finality)) + 
    geom_violin(aes(color=language), show.legend=F) + 
    labs(title="", x="Language", y="Head-finality", color="") + 
    theme(legend.position="bottom") +
    facet_grid(modality~language) + 
    gg_theme

#ggsave("plots/total_dependencies_all_languages.png", plot=all.dependencies, device="png", dpi = 300, width=10)
gg.violin.overall.headedness

```

### Trends by sentence length

```{r graphs_all_headedness}

gg.all.headedness <- ggplot(df.all.sampled.diff.ent, aes(x=num_deps, y=head_finality)) + 
    geom_point(aes(color=language), show.legend=F) + 
    geom_smooth(method="lm", formula=y~x, color="#666666") + 
    labs(title="", x="Sentence Length", y="Headedness", color="") + 
    theme(legend.position="bottom") +
    facet_grid(modality~language) + 
    gg_theme

#ggsave("plots/total_dependencies_all_languages.png", plot=all.dependencies, device="png", dpi = 300, width=10)
gg.all.headedness

```

# Entropy

## Distributions (average entropy at each sentence length)

JA, EN, and KO have consistently low entropy; FR, RU, IT, and TR are more variable. Big difference between written and spoken French.

```{r graphs_entropy_trans}

gg.violin.overall.entropy.trans <- ggplot(df.all.sampled.diff.ent, aes(x=language, y=entropyTrans)) + 
    geom_violin(aes(color=language, fill=language), alpha=0.5, show.legend=F) + 
    labs(title="", x="Language", y="Entropy of S, V, O", color="") + 
    theme(legend.position="bottom") +
    facet_wrap(.~modality, ncol=1) + 
    scale_color_ghibli_d("MononokeMedium") +
    scale_fill_ghibli_d("MononokeMedium") +
    gg_theme

ggsave("plots/entropy_distributions_trans.png", plot=gg.violin.overall.entropy.trans, device="png", dpi = 300, width=10)
gg.violin.overall.entropy.trans

```

## Entropy LMs

```{r entropy_regression}

df.entropy.lm <- df.all.sampled.diff.ent %>% 
  group_by(language, num_deps, corpus, modality, id) %>% 
  summarize(entropyTrans=mean(entropyTrans)) %>%
  mutate(baslineWritten=ifelse(modality=="written", 0, 1))

lm.entropy.fr <- lmer(entropyTrans ~ num_deps * baslineWritten + (1 | corpus), data=df.entropy.lm %>% filter(language=="fr"))
lm.entropy.ru <- lmer(entropyTrans ~ num_deps * baslineWritten + (1 | corpus), data=df.entropy.lm %>% filter(language=="ru"))
lm.entropy.en <- lmer(entropyTrans ~ num_deps * baslineWritten + (1 | corpus), data=df.entropy.lm %>% filter(language=="en"))
lm.entropy.ja <- lmer(entropyTrans ~ num_deps * baslineWritten + (1 | corpus), data=df.entropy.lm %>% filter(language=="ja"))
lm.entropy.ko <- lmer(entropyTrans ~ num_deps * baslineWritten + (1 | corpus), data=df.entropy.lm %>% filter(language=="ko"))
lm.entropy.it <- lmer(entropyTrans ~ num_deps * baslineWritten + (1 | corpus), data=df.entropy.lm %>% filter(language=="it"))
lm.entropy.tr <- lmer(entropyTrans ~ num_deps * baslineWritten + (1 | corpus), data=df.entropy.lm %>% filter(language=="tr"))

summary(lm.entropy.fr)
summary(lm.entropy.ru)
summary(lm.entropy.en)
summary(lm.entropy.ja)
summary(lm.entropy.ko)
summary(lm.entropy.tr)

```


## Entropy by sentence length

Entropy generally decreases as sentence length increases, with Korean and Japanese as exceptions.

```{r graphs_youdepp_entropy_svo}

gg.youdepp.entropy.trans <- ggplot(df.all.sampled.diff.ent %>% group_by(modality, language, corpus, num_deps) %>% summarize(entropyTrans=mean(entropyTrans)), aes(x=num_deps, y=entropyTrans)) + 
    geom_point(alpha=0.25, show.legend=F) + 
    geom_smooth(aes(color=language), method="lm", formula=y~x) + 
    labs(title="", x="Sentence Length", y="Entropy of S, V, O", color="") + 
    theme(legend.position="bottom") +
    facet_grid(modality~language) +
    #scale_color_ghibli_d("MononokeMedium") +
    #scale_fill_ghibli_d("MononokeMedium") +
    gg_theme

#ggsave("plots/entropy_by_length.png", plot=gg.youdepp.entropy.trans, device="png", dpi = 300, width=10)
gg.youdepp.entropy.trans

```

# Linear Models

## YouDePP

```{r lmer_by_sentence_youdepp} 

# df.youdepp.lm.bysent.diff <- df.youdepp.stats.sampled.diff.ent %>% 
#   select(language, corpus, id, num_deps, totalDLDiff, averageDLDiff, averageDLSLDiff, head_finality, entropyTrans) %>% 
#   mutate(language=factor(language), corpus=factor(corpus), id=factor(id), s_len_sq=num_deps^2)
# 
# lm.features.youdep.bysent.diff <- lmer(totalDLDiff ~ 
#                                          scale(s_len_sq) + scale(head_finality) + scale(entropyTrans) + 
#                                          scale(entropyTrans):scale(head_finality) + 
#                                          scale(s_len_sq):scale(entropyTrans) + 
#                                          (1 | id) + (1 | language) + (1 | corpus),# + 
#                                          #(1 + language | entropyTrans)  + (1 + corpus | entropyTrans) + 
#                                          #(1 + language | head_finality) + (1 + corpus | head_finality),
#                                        data=df.youdepp.lm.bysent.diff,
#                                        control=lmerControl(optimizer="bobyqa"))
# 
# summary(lm.features.youdep.bysent.diff)
# 
# # How do predictions change with higher entropy + longer sentences? Priors are different...do you KNOW you're in a longer sentence? (E.g. news vs. writing vs. comedy...)

```

## UD

```{r lmer_by_sentence_ud} 

# df.ud.lm.bysent.diff <- df.ud.stats.sampled.diff.ent %>% 
#   select(language, corpus, id, num_deps, totalDLDiff, averageDLDiff, averageDLSLDiff, head_finality, entropyTrans) %>% 
#   mutate(language=factor(language), corpus=factor(corpus), id=factor(id), s_len_sq=num_deps^2)
# 
# lm.features.youdep.bysent.diff <- lmer(totalDLDiff ~ 
#                                          scale(s_len_sq) + scale(head_finality) + scale(entropyTrans) + 
#                                          scale(entropyTrans):scale(head_finality) + 
#                                          scale(s_len_sq):scale(entropyTrans) + 
#                                          (1 | id) + (1 | language), data=df.ud.lm.bysent.diff,
#                               control=lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=100000)))
# 
# summary(lm.features.youdep.bysent.diff)

```

## Written & spoken

### Legacy setup (no longer sampling!)

```{r cross_modal_setup}

# # Sample wth replacement multiple times & take average at each language x corpus x length  combination
# # 100 averages
# 
# languages <- c("en", "fr", "it", "tr", "ja", "ko", "ru")
# df.all.diff.sampled <- data.frame()
# 
# # Suppress summarise info
# options(dplyr.summarise.inform = FALSE)
# 
# for (i in languages) {
#   
#   df.youdepp.temp.l <- df.youdepp.stats.sampled.diff.ent %>% filter(language==i)
#   df.ud.temp.l      <- df.ud.stats.sampled.diff.ent %>% filter(language==i)
#   
#   df.youdepp.temp.l <- df.youdepp.temp.l %>% filter(num_deps %in% lengths)
#   df.ud.temp.l      <- df.ud.temp.l %>% filter(num_deps %in% lengths)
#   
#   lengths <- intersect(unique(df.youdepp.temp.l$num_deps), unique(df.ud.temp.l$num_deps))
#   
#   for(j in lengths) {
#   
#     df.youdepp.temp.l.n <- df.youdepp.temp.l %>% filter(num_deps==j)
#     df.ud.temp.l.n      <- df.ud.temp.l %>% filter(num_deps==j)
#   
#     total_n  <- nrow(df.youdepp.temp.l) + nrow(df.ud.temp.l)
#     subset_n <- nrow(df.youdepp.temp.l.n) + nrow(df.ud.temp.l.n)
#     prop_l_n <- subset_n/total_n
#     
#     for (k in 1:round(10000 * prop_l_n)) {
#       
#       df.youdepp.temp.sampled <- df.youdepp.temp.l.n %>% 
#         group_by(language, num_deps) %>% 
#         sample_n(n(), replace=T) %>%
#         summarize(totalDLDiff_s = mean(totalDLDiff), 
#                   averageDLDiff_s = mean(averageDLDiff), 
#                   averageDLSLDiff_s = mean(averageDLSLDiff), 
#                   head_finality_s = mean(head_finality), 
#                   entropyTrans_s = mean(entropyTrans)) %>% ungroup()
#       
#       df.ud.temp.sampled <- df.ud.temp.l.n %>% 
#         group_by(language, num_deps) %>% 
#         sample_n(n(), replace=T) %>%
#         summarize(totalDLDiff_w = mean(totalDLDiff), 
#                   averageDLDiff_w = mean(averageDLDiff), 
#                   averageDLSLDiff_w = mean(averageDLSLDiff), 
#                   head_finality_w = mean(head_finality), 
#                   entropyTrans_w = mean(entropyTrans)) %>% ungroup()
#       
#       # Written - spoken
#       df.all.diff <- merge(df.youdepp.temp.sampled, df.ud.temp.sampled, by=c("language", "num_deps")) %>%
#         mutate(totalDLDiff=(totalDLDiff_w-totalDLDiff_s), 
#                averageDLDiff=(averageDLDiff_w-averageDLDiff_s), 
#                averageDLSLDiff=(averageDLSLDiff_w-averageDLSLDiff_s), 
#                head_finality=(head_finality_w-head_finality_s), 
#                entropyTrans=sum(entropyTrans_w-entropyTrans_s)) %>%
#         select(language, num_deps, totalDLDiff, averageDLDiff, averageDLSLDiff, head_finality, entropyTrans)
#       
#       df.all.diff.sampled <- bind_rows(df.all.diff.sampled, df.all.diff)
#     }
#   }
# }
# 
# df.all.diff.sampled %>% group_by(language) %>% summarize(count=n(), avgDiffTot=mean(totalDLDiff), avgDiffHead=mean(head_finality), avgDiffEnt=mean(entropyTrans))
# 
# ggplot(df.all.diff.sampled, aes(x=factor(num_deps), y=totalDLDiff)) +
#   geom_violin() + 
#   facet_wrap(.~language)

```

```{r written_spoken_regression_sampled}

# df.all.lm.bysent.diff <- df.all.diff.sampled %>% 
#   select(language, num_deps, totalDLDiff, averageDLDiff, averageDLSLDiff, head_finality, entropyTrans) %>% 
#   mutate(language=factor(language), s_len_sq=num_deps^2)
# 
# lm.features.all.bysent.diff <- lmer(totalDLDiff ~ 
#                                          scale(s_len_sq) + scale(head_finality) + scale(entropyTrans) + 
#                                          scale(entropyTrans):scale(head_finality) + 
#                                          scale(s_len_sq):scale(entropyTrans) + 
#                                          (1 | language), data=df.all.lm.bysent.diff,
#                               control=lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=100000)))
# 
# summary(lm.features.all.bysent.diff)
# 
# #(rand_w - obs_w) - (rand_s - obs_s)
# #rand - obs bigger = more minimization within modality
# # diff_w - diff_s bigger = more minimization IN WRITING

```


### Full regression setup

```{r regression_full_setup}

df.all.stats.diff <- bind_rows(list("spoken"=df.youdepp.stats.sampled.diff.ent, "written"=df.ud.stats.sampled.diff.ent), .id="modality")

# Clean up data for regression
df.all.lm <- df.all.stats.diff %>% 
  mutate(spoken            = ifelse(modality=="written", 0, 1), 
         written           = ifelse(modality=="written", 1, 0), 
         s_len_sq          = num_deps^2) %>%
  rename(difference_score = totalDLDiff) %>%
  select(modality, language, corpus, id, num_deps,
         s_len_sq, difference_score, head_finality, entropyTrans)

# Take the average difference of each sentence for easier regression calculation
# (Doesn't change results, but should maybe use full data later?)
df.all.avg.lm <- df.all.lm %>% 
  group_by(id) %>% 
  summarize(language         = head(language, 1),
            corpus           = head(corpus, 1),
            modality         = head(modality, 1),
            num_deps         = head(num_deps, 1),
            head_finality    = mean(head_finality), 
            entropyTrans     = mean(entropyTrans), 
            difference_score = mean(difference_score)) %>% 
  ungroup() %>%
  mutate(s_len_sq = num_deps^2,
         spoken   = ifelse(modality=="written", 0, 1), 
         written  = ifelse(modality=="written", 1, 0),
         s_len_sq_scaled   = scale(s_len_sq),
         headedness_scaled = scale(head_finality),
         entropy_scaled    = scale(entropyTrans))

```

### Baseline written

```{r regression_full_baselineWritten}

lm.diff.avg.baseWritten <- lmer(difference_score ~ 
      # Main effects
      spoken + s_len_sq_scaled + headedness_scaled + entropy_scaled + 
      # Interactions
      headedness_scaled:entropy_scaled + 
      s_len_sq_scaled:entropy_scaled + 
      s_len_sq_scaled:headedness_scaled + 
      # 3rd order
      s_len_sq_scaled:headedness_scaled:entropy_scaled + 
      # Interactions with baseline
      spoken:s_len_sq_scaled + 
      spoken:headedness_scaled + spoken:entropy_scaled + 
      # 3rd order interactions
      spoken:headedness_scaled:entropy_scaled + 
      spoken:s_len_sq_scaled:headedness_scaled + 
      spoken:s_len_sq_scaled:entropy_scaled + 
      # Technically this one would also be interesting, but...
      #spoken:s_len_sq_scaled:entropy_scaled:headedness_scaled + 
      #(1 | id) + 
      (1 | language) + (1 | corpus), data=df.all.avg.lm,
      control=lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=100000)))

conf.lm.diff.avg.baseWritten <- confint(lm.diff.avg.baseWritten, parm="beta_", method="Wald")

summary(lm.diff.avg.baseWritten)
conf.lm.diff.avg.baseWritten

#df.all.avg.lm %>% group_by(corpus) %>% summarize(count=n())

```

### Baseline spoken

```{r regression_full_baselineSpoken}

lm.diff.avg.baseSpoken <- lmer(difference_score ~ 
      # Main effects
      written + s_len_sq_scaled + headedness_scaled + entropy_scaled + 
      # Main interactions
      headedness_scaled:entropy_scaled + 
      s_len_sq_scaled:entropy_scaled + 
      s_len_sq_scaled:headedness_scaled + 
      # 3rd order
      s_len_sq_scaled:headedness_scaled:entropy_scaled + 
      # Interactions with baseline
      written:s_len_sq_scaled + 
      written:headedness_scaled + written:entropy_scaled + 
      # 3rd order interactions
      written:headedness_scaled:entropy_scaled + 
      written:s_len_sq_scaled:headedness_scaled + 
      written:s_len_sq_scaled:entropy_scaled + 
      # 4th order?????
      #written:s_len_sq_scaled:entropy_scaled:headedness_scaled + 
      #(1 | id) + 
      (1 | language) + (1 | corpus), data=df.all.avg.lm,
      control=lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=100000)))

summary(lm.diff.avg.baseSpoken)
conf.lm.diff.avg.baseSpoken <- confint(lm.diff.avg.baseSpoken, parm="beta_", method="Wald")
conf.lm.diff.avg.baseSpoken

```

## Model predictions

### Setup

```{r set_up_predictions}

# Separate data sets for within-baseline calculations
df.all.avg.lm.spoken  <- df.all.avg.lm %>% filter(spoken == 1)
df.all.avg.lm.written <- df.all.avg.lm %>% filter(spoken == 0)

# Calculate useful variables for expanding
hf_min.spoken  = min(df.all.avg.lm.spoken$headedness_scaled)
hf_max.spoken  = max(df.all.avg.lm.spoken$headedness_scaled)
hf_mean.spoken = mean(df.all.avg.lm.spoken$headedness_scaled)
hf_sd.spoken   = sd(df.all.avg.lm.spoken$headedness_scaled)

ent_min.spoken  = min(df.all.avg.lm.spoken$entropy_scaled)
ent_max.spoken  = max(df.all.avg.lm.spoken$entropy_scaled)
ent_mean.spoken = mean(df.all.avg.lm.spoken$entropy_scaled)
ent_sd.spoken   = sd(df.all.avg.lm.spoken$entropy_scaled)

len_min.spoken  <- min(df.all.avg.lm.spoken$s_len_sq_scaled)
len_mean.spoken <- mean(df.all.avg.lm.spoken$s_len_sq_scaled)
len_max.spoken  <- max(df.all.avg.lm.spoken$s_len_sq_scaled)
len_sd.spoken   <- sd(df.all.avg.lm.spoken$s_len_sq_scaled)

hf_min.written  = min(df.all.avg.lm.written$headedness_scaled)
hf_max.written  = max(df.all.avg.lm.written$headedness_scaled)
hf_mean.written = mean(df.all.avg.lm.written$headedness_scaled)
hf_sd.written   = sd(df.all.avg.lm.written$headedness_scaled)

ent_min.written  = min(df.all.avg.lm.written$entropy_scaled)
ent_max.written  = max(df.all.avg.lm.written$entropy_scaled)
ent_mean.written = mean(df.all.avg.lm.written$entropy_scaled)
ent_sd.written   = sd(df.all.avg.lm.written$entropy_scaled)

len_min.written  <- min(df.all.avg.lm.written$s_len_sq_scaled)
len_mean.written <- mean(df.all.avg.lm.written$s_len_sq_scaled)
len_max.written  <- max(df.all.avg.lm.written$s_len_sq_scaled)
len_sd.written   <- sd(df.all.avg.lm.written$s_len_sq_scaled)

```

### Headedness vs. entropy

```{r set_up_predictions_headednessVsEntropy}

# Spoken prediction data
df.predict.headVent.spoken <- expand.grid(
  headedness_scaled = c(hf_min.spoken, hf_mean.spoken, hf_max.spoken), 
  s_len_sq_scaled   = unique(df.all.avg.lm.spoken$s_len_sq_scaled),
  entropy_scaled    = c(ent_min.spoken, ent_mean.spoken, ent_max.spoken), 
  spoken            = c(1),
  language          = unique(df.all.avg.lm.spoken$language),  
  corpus            = unique(df.all.avg.lm.spoken$corpus)
)

# Written prediction data
df.predict.headVent.written <- expand.grid(
  headedness_scaled = c(hf_min.written, hf_mean.written, hf_max.written), 
  s_len_sq_scaled   = unique(df.all.avg.lm.written$s_len_sq_scaled),
  entropy_scaled    = c(ent_min.written, ent_mean.written, ent_max.written), 
  spoken            = c(0), 
  language          = unique(df.all.avg.lm.written$language),  
  corpus            = unique(df.all.avg.lm.written$corpus)
)

# Predict spoken
df.predict.headVent.spoken$difference_predicted <- predict(lm.diff.avg.baseWritten, newdata=df.predict.headVent.spoken)

# Predict written
df.predict.headVent.written$difference_predicted <- predict(lm.diff.avg.baseWritten, newdata=df.predict.headVent.written)

```

```{r clean_predictions_headednessVsEntropy}

# Give our entropy and headedness categories nice names
df.predict.headVent.spoken <- df.predict.headVent.spoken %>% 
  mutate(headednessCat = case_when(headedness_scaled == hf_min.spoken ~ "Minimum", 
                                   headedness_scaled == hf_max.spoken ~ "Maximum", 
                                   TRUE ~ "Mean"),
         entropyCat    = case_when(entropy_scaled == ent_min.spoken ~ "Minimum",
                                   entropy_scaled == ent_max.spoken ~ "Maximum", 
                                   TRUE ~ "Mean"),
         modality = ifelse(spoken==0, "Written (UD)", "Spoken (YouTube)"))
 
df.predict.headVent.written <- df.predict.headVent.written %>% 
  mutate(headednessCat = case_when(headedness_scaled == hf_min.written ~ "Minimum", 
                                   headedness_scaled == hf_max.written ~ "Maximum", 
                                   TRUE ~ "Mean"),
         entropyCat    = case_when(entropy_scaled == ent_min.written ~ "Minimum",
                                   entropy_scaled == ent_max.written ~ "Maximum", 
                                   TRUE ~ "Mean"),
         modality = ifelse(spoken==0, "Written (UD)", "Spoken (YouTube)"))

# Combine dataframes
df.predict.headVent <- bind_rows(df.predict.headVent.spoken, df.predict.headVent.written)

# Collapse unnecessary predictors
df.predict.headVent <- df.predict.headVent %>% 
  group_by(s_len_sq_scaled, modality, headednessCat, entropyCat) %>% 
  summarize(difference_predicted = mean(difference_predicted)) %>%
  mutate(modality = factor(modality, levels=c("Written (UD)", "Spoken (YouTube)")))

```

```{r clean_observed_headednessVsEntropy}

# Spoken
df.observed.headVent.spoken <- df.all.avg.lm.spoken %>%
  mutate(entropyCat = case_when(entropy_scaled <= (ent_mean.spoken - ent_sd.spoken/3) ~ "Minimum",
                                entropy_scaled >= (ent_mean.spoken + ent_sd.spoken/3) ~ "Maximum",
                                TRUE ~ "Mean"),
         headednessCat = case_when(headedness_scaled <= (hf_mean.spoken - hf_sd.spoken) ~ "Minimum",
                                headedness_scaled >= (hf_mean.spoken + hf_sd.spoken) ~ "Maximum",
                                TRUE ~ "Mean"),
         modality = ifelse(spoken==0, "Written (UD)", "Spoken (YouTube)")) %>% 
  group_by(s_len_sq_scaled, modality, num_deps, headednessCat, entropyCat) %>% 
  summarize(difference_score = mean(difference_score))

# Written
df.observed.headVent.written <- df.all.avg.lm.written %>%
  mutate(entropyCat = case_when(entropy_scaled <= (ent_mean.written - ent_sd.written/3) ~ "Minimum",
                                entropy_scaled >= (ent_mean.written + ent_sd.written/3) ~ "Maximum",
                                TRUE ~ "Mean"),
         headednessCat = case_when(headedness_scaled <= (hf_mean.written - hf_sd.written) ~ "Minimum",
                                headedness_scaled >= (hf_mean.written + hf_sd.written) ~ "Maximum",
                                TRUE ~ "Mean"),
         modality = ifelse(spoken==0, "Written (UD)", "Spoken (YouTube)")) %>% 
  group_by(s_len_sq_scaled, num_deps, modality, headednessCat, entropyCat) %>% 
  summarize(difference_score = mean(difference_score))
  
df.observed.headVent <- bind_rows(df.observed.headVent.spoken, df.observed.headVent.written)  %>%
  mutate(modality = factor(modality, levels=c("Written (UD)", "Spoken (YouTube)")))

```

```{r graph_entropy}

df.predict.headVent$entropyCat  <- factor(df.predict.headVent$entropyCat, 
                                          levels=c("Minimum", "Mean", "Maximum"))
df.observed.headVent$entropyCat <- factor(df.observed.headVent$entropyCat, 
                                          levels=c("Minimum", "Mean", "Maximum"))

model.entropy.g <- ggplot(df.predict.headVent %>% 
                            group_by(modality, s_len_sq_scaled, entropyCat) %>% 
                            summarize(difference_predicted = mean(difference_predicted)), 
                          aes(x=s_len_sq_scaled, y=difference_predicted, color=entropyCat))

# Observed data
model.entropy.g <- model.entropy.g +
  geom_point(data=df.observed.headVent,
             inherit.aes=FALSE, 
             aes(x=(s_len_sq_scaled), y=(difference_score), 
                 color=entropyCat), shape="triangle", alpha=0.5, size=1.5)
 
# Prediction line plot
model.entropy.g <- model.entropy.g +
  geom_line(aes(x=(s_len_sq_scaled), y=(difference_predicted), 
                color=entropyCat), linetype="dashed", alpha=0.75, size=1.5)

# Theme
model.entropy.g <- model.entropy.g +
  scale_color_manual(values=c("#61c75d", "#0086d6", "#001C49", "#61c75d", "#0086d6", "#001C49")) +
  geom_hline(aes(yintercept=0), linetype="dashed", width=1.5) + 
  coord_cartesian(ylim=c(-5, 20)) + 
  facet_wrap(. ~ modality) +
  labs(title="Difference scores by length and entropy", x="Scaled sentence Length", y="Random - Observed", color="Entropy") + 
  gg_theme + 
  theme(legend.position="right")

ggsave("plots/model_predictions_entropy.png", plot=model.entropy.g, device="png", dpi = 300, width=10)
model.entropy.g

```

```{r graph_headedness}

df.predict.headVent$headednessCat  <- factor(df.predict.headVent$headednessCat, 
                                          levels=c("Minimum", "Mean", "Maximum"))
df.observed.headVent$headednessCat <- factor(df.observed.headVent$headednessCat, 
                                          levels=c("Minimum", "Mean", "Maximum"))

model.headedness.g <- ggplot(df.predict.headVent %>% 
                            group_by(modality, s_len_sq_scaled, headednessCat) %>% 
                            summarize(difference_predicted = mean(difference_predicted)), 
                          aes(x=s_len_sq_scaled, y=difference_predicted, color=headednessCat))

# Observed data
model.headedness.g <- model.headedness.g +
  geom_point(data=df.observed.headVent,
             inherit.aes=FALSE, 
             aes(x=(s_len_sq_scaled), y=(difference_score), 
                 color=headednessCat), shape="triangle", alpha=0.5, size=1.5)
 
# Prediction line plot
model.headedness.g <- model.headedness.g +
  geom_line(aes(x=(s_len_sq_scaled), y=(difference_predicted), 
                color=headednessCat), linetype="dashed", alpha=0.75, size=1.5)

# Theme
model.headedness.g <- model.headedness.g +
  scale_color_manual(values=c("#61c75d", "#0086d6", "#001C49", "#61c75d", "#0086d6", "#001C49")) +
  geom_hline(aes(yintercept=0), linetype="dashed", width=1.5) + 
  coord_cartesian(ylim=c(-5, 20)) + 
  facet_wrap(. ~ modality) +
  labs(title="Difference scores by length and headedness", x="Scaled sentence length", y="Random - Observed", color="Head-finality") + 
  gg_theme + 
  theme(legend.position="right")

ggsave("plots/model_predictions_headedness.png", plot=model.headedness.g, device="png", dpi = 300, width=10)
model.headedness.g

```

```{r arrange_graphs_headent}

gg.headedness_entropy.arranged <- ggarrange(model.entropy.g, model.headedness.g, nrow = 2)
ggsave("plots/model_predictions_headedness_entropy.png", plot=gg.headedness_entropy.arranged, device="png", dpi = 300, width=9, height=9)
gg.headedness_entropy.arranged
```


### Headedness together with entropy

```{r set_up_predictions_headednessAndEntropy}

# Spoken prediction data
df.predict.headANDent.spoken <- expand.grid(
  headedness_scaled = c(hf_min.spoken, hf_mean.spoken, hf_max.spoken), 
  s_len_sq_scaled   = c(len_min.spoken + 0.5, len_mean.spoken, len_max.spoken - 4),
  entropy_scaled    = seq(ent_min.spoken, ent_max.spoken, by=0.1), 
  spoken            = c(1),
  language          = unique(df.all.avg.lm.spoken$language),  
  corpus            = unique(df.all.avg.lm.spoken$corpus)
) 

# Written prediction data
df.predict.headANDent.written <- expand.grid(
  headedness_scaled = c(hf_min.written, hf_mean.written, hf_max.written), 
  s_len_sq_scaled   = c(len_min.written + 0.5, len_mean.written, len_max.written - 4),
  entropy_scaled    = seq(ent_min.written, ent_max.written, by=0.1), 
  spoken            = c(0), 
  language          = unique(df.all.avg.lm.written$language),  
  corpus            = unique(df.all.avg.lm.written$corpus)
) 

# Predict spoken
df.predict.headANDent.spoken$difference_predicted <- predict(lm.diff.avg.baseWritten, newdata=df.predict.headANDent.spoken)

# Predict written
df.predict.headANDent.written$difference_predicted <- predict(lm.diff.avg.baseWritten, newdata=df.predict.headANDent.written)

```

```{r clean_predictions_headednessAndEntropy}

# Give our entropy and headedness categories nice names
df.predict.headANDent.spoken <- df.predict.headANDent.spoken %>% 
  mutate(headednessCat = case_when(headedness_scaled == hf_min.spoken ~ "Minimum", 
                                   headedness_scaled == hf_max.spoken ~ "Maximum", 
                                   TRUE ~ "Mean"),
         lengthCat    = case_when(s_len_sq_scaled == len_min.spoken + 0.5 ~ "Min length",
                                   s_len_sq_scaled == len_max.spoken - 4 ~ "Max length", 
                                   TRUE ~ "Mean length"),
         modality = ifelse(spoken==0, "Written (UD)", "Spoken\n(YouTube)"))
 
df.predict.headANDent.written <- df.predict.headANDent.written %>% 
  mutate(headednessCat = case_when(headedness_scaled == hf_min.written ~ "Minimum", 
                                   headedness_scaled == hf_max.written ~ "Maximum", 
                                   TRUE ~ "Mean"),
         lengthCat    = case_when(s_len_sq_scaled == len_min.written + 0.5 ~ "Min length",
                                   s_len_sq_scaled == len_max.written - 4 ~ "Max length", 
                                   TRUE ~ "Mean length"),
         modality = ifelse(spoken==0, "Written (UD)", "Spoken\n(YouTube)"))

# Combine dataframes
df.predict.headANDent <- bind_rows(df.predict.headANDent.spoken, df.predict.headANDent.written)

# Collapse unnecessary predictors
df.predict.headANDent <- df.predict.headANDent %>% 
  group_by(entropy_scaled, modality, headednessCat, lengthCat) %>% 
  summarize(difference_predicted = mean(difference_predicted))

  
```

```{r clean_observed_headednessAndEntropy}

# Spoken
df.observed.headANDent.spoken <- df.all.avg.lm.spoken %>%
  mutate(lengthCat = case_when(s_len_sq_scaled <= (len_mean.spoken - len_sd.spoken/2) ~ "Min length",
                                s_len_sq_scaled >= (len_mean.spoken + len_sd.spoken) ~ "Max length",
                                TRUE ~ "Mean length"),
         headednessCat = case_when(headedness_scaled <= (hf_mean.spoken - hf_sd.spoken) ~ "Minimum",
                                headedness_scaled >= (hf_mean.spoken + hf_sd.spoken) ~ "Maximum",
                                TRUE ~ "Mean"),
         modality = ifelse(spoken==0, "Written (UD)", "Spoken\n(YouTube)")) %>% 
  group_by(entropy_scaled, modality, num_deps, headednessCat, lengthCat) %>% 
  summarize(difference_score = mean(difference_score))

# Written
df.observed.headANDent.written <- df.all.avg.lm.written %>%
  mutate(lengthCat = case_when(s_len_sq_scaled <= (len_mean.written - len_sd.written/2) ~ "Min length",
                                s_len_sq_scaled >= (len_mean.written + len_sd.written/3) ~ "Max length",
                                TRUE ~ "Mean length"),
         headednessCat = case_when(headedness_scaled <= (hf_mean.written - hf_sd.written) ~ "Minimum",
                                headedness_scaled >= (hf_mean.written + hf_sd.written) ~ "Maximum",
                                TRUE ~ "Mean"),
         modality = ifelse(spoken==0, "Written (UD)", "Spoken\n(YouTube)")) %>% 
  group_by(entropy_scaled, modality, num_deps, headednessCat, lengthCat) %>% 
  summarize(difference_score = mean(difference_score))
  
df.observed.headANDent <- bind_rows(df.observed.headANDent.spoken, df.observed.headANDent.written)

```

```{r graph_headedness_entropy}

df.predict.headANDent$modality       <- factor(df.predict.headANDent$modality, 
                                               levels=c("Written (UD)", "Spoken\n(YouTube)"))
df.predict.headANDent$lengthCat      <- factor(df.predict.headANDent$lengthCat, 
                                               levels=c("Min length", "Mean length", "Max length"))
df.predict.headANDent$headednessCat  <- factor(df.predict.headANDent$headednessCat, 
                                               levels=c("Minimum", "Mean", "Maximum"))

df.observed.headANDent$modality      <- factor(df.observed.headANDent$modality, 
                                               levels=c("Written (UD)", "Spoken\n(YouTube)"))
df.observed.headANDent$lengthCat     <- factor(df.observed.headANDent$lengthCat, 
                                               levels=c("Min length", "Mean length", "Max length"))
df.observed.headANDent$headednessCat <- factor(df.observed.headANDent$headednessCat, 
                                               levels=c("Minimum", "Mean", "Maximum"))


model.headANDent.g <- ggplot(df.predict.headANDent,
                          aes(x=entropy_scaled, y=difference_predicted))

# Observed data
model.headANDent.g <- model.headANDent.g +
  geom_point(data=df.observed.headANDent %>%
               group_by(modality, entropy_scaled, headednessCat, lengthCat) %>%
               summarize(difference_score = mean(difference_score)),
             inherit.aes=FALSE,
             aes(x=(entropy_scaled), y=(difference_score),
                 color=headednessCat), shape="triangle", alpha=0.5, size=1.5)

# Prediction line plot
model.headANDent.g <- model.headANDent.g +
  geom_line(aes(x=(entropy_scaled), y=(difference_predicted), 
                color=headednessCat), linetype="dashed", alpha=0.9, size=1.5)

# Theme
model.headANDent.g <- model.headANDent.g +
  scale_color_manual(values=c("#61c75d", "#0086d6", "#001C49", "#61c75d", "#0086d6", "#001C49")) +
  #geom_hline(aes(yintercept=0), linetype="dashed", width=1.5) + 
  coord_cartesian(ylim=c(-2, 12)) + 
  facet_grid(modality ~ lengthCat) +
  labs(title="", x="Entropy", y="Random - Observed", color="Head-finality") + 
  gg_theme

ggsave("plots/model_predictions_headednessEntropy.png", plot=model.headANDent.g, device="png", dpi = 300, width=8)
model.headANDent.g

```

### Written vs. spoken difference scores

#### Headedness vs. entropy

```{r merge_and_diff_headVent}

df.predict.headVent.diff.spoken <- df.predict.headVent %>% 
  ungroup() %>%
  filter(modality == "Spoken (YouTube)") %>% 
  select(-modality) %>% 
  rename(difference_predicted_spoken = difference_predicted) %>%
  mutate(s_len_sq_scaled = round(s_len_sq_scaled, 2))
  
df.predict.headVent.diff.written <- df.predict.headVent  %>% 
  ungroup() %>%
  filter(modality == "Written (UD)") %>%
  select(-modality) %>% 
  rename(difference_predicted_written = difference_predicted) %>%
  mutate(s_len_sq_scaled = round(s_len_sq_scaled, 2))

df.predict.headVent.diff <- merge(df.predict.headVent.diff.spoken, df.predict.headVent.diff.written) %>%
  mutate(difference_score = difference_predicted_spoken - difference_predicted_written)


## Observed data

df.observed.headVent.diff.spoken <- df.observed.headVent %>% 
  ungroup() %>%
  filter(modality == "Spoken (YouTube)") %>% 
  select(-modality) %>% 
  rename(difference_score_spoken = difference_score) %>%
  mutate(s_len_sq_scaled = round(s_len_sq_scaled, 2))
  
df.observed.headVent.diff.written <- df.observed.headVent  %>% 
  ungroup() %>%
  filter(modality == "Written (UD)") %>%
  select(-modality) %>% 
  rename(difference_score_written = difference_score, s_len_sq_scaled_written = s_len_sq_scaled) %>%
  mutate(s_len_sq_scaled_written = round(s_len_sq_scaled_written, 2))

df.observed.headVent.diff <- merge(df.observed.headVent.diff.spoken, df.observed.headVent.diff.written) %>%
  mutate(difference_score = difference_score_spoken - difference_score_written)


```

```{r graph_entropy_difference}

df.predict.headVent.diff$entropyCat  <- factor(df.predict.headVent.diff$entropyCat, 
                                          levels=c("Minimum", "Mean", "Maximum"))
df.observed.headVent.diff$entropyCat <- factor(df.observed.headVent.diff$entropyCat, 
                                          levels=c("Minimum", "Mean", "Maximum"))

model.entropy.diff.g <- ggplot(df.predict.headVent.diff %>% 
                            group_by(s_len_sq_scaled, entropyCat) %>% 
                            summarize(difference_score = mean(difference_score)), 
                          aes(x=s_len_sq_scaled, y=difference_score, color=entropyCat))

# Observed data
model.entropy.diff.g <- model.entropy.diff.g +
  geom_point(data=df.observed.headVent.diff,
             inherit.aes=FALSE, 
             aes(x=(s_len_sq_scaled), y=(difference_score), 
                 color=entropyCat), shape="triangle", alpha=0.5, size=1.5)
 
# Prediction line plot
model.entropy.diff.g <- model.entropy.diff.g +
  geom_line(aes(x=(s_len_sq_scaled), y=(difference_score), 
                color=entropyCat), linetype="dashed", alpha=0.75, size=1.5)

# Theme
model.entropy.diff.g <- model.entropy.diff.g +
  scale_color_manual(values=c("#61c75d", "#0086d6", "#001C49")) +
  geom_hline(aes(yintercept=0), linetype="dashed", width=1.5) + 
  labs(title="Difference scores by length and entropy", x="Scaled sentence Length", y="Written - Spoken", color="Entropy") + 
  coord_cartesian(ylim=c(-5, 5)) + 
  gg_theme + 
  theme(legend.position="right")

ggsave("plots/model_predictions_entropy_diff.png", plot=model.entropy.diff.g, device="png", dpi = 300, width=10)
model.entropy.diff.g

```

```{r graph_headedness_difference}

df.predict.headVent.diff$headednessCat  <- factor(df.predict.headVent.diff$headednessCat, 
                                          levels=c("Minimum", "Mean", "Maximum"))
df.observed.headVent.diff$headednessCat <- factor(df.observed.headVent.diff$headednessCat, 
                                          levels=c("Minimum", "Mean", "Maximum"))

model.headedness.diff.g <- ggplot(df.predict.headVent.diff %>% 
                            group_by(s_len_sq_scaled, headednessCat) %>% 
                            summarize(difference_score = mean(difference_score)), 
                          aes(x=s_len_sq_scaled, y=difference_score, color=headednessCat))

# Observed data
model.headedness.diff.g <- model.headedness.diff.g +
  geom_point(data=df.observed.headVent.diff,
             inherit.aes=FALSE, 
             aes(x=(s_len_sq_scaled), y=(difference_score), 
                 color=headednessCat), shape="triangle", alpha=0.5, size=1.5)
 
# Prediction line plot
model.headedness.diff.g <- model.headedness.diff.g +
  geom_line(aes(x=(s_len_sq_scaled), y=(difference_score), 
                color=headednessCat), linetype="dashed", alpha=0.75, size=1.5)

# Theme
model.headedness.diff.g <- model.headedness.diff.g +
  scale_color_manual(values=c("#61c75d", "#0086d6", "#001C49")) +
  geom_hline(aes(yintercept=0), linetype="dashed", width=1.5) + 
  labs(title="Difference scores by length and headedness", x="Scaled sentence Length", y="Written - Spoken", color="Head-finality") + 
  coord_cartesian(ylim=c(-5, 5)) + 
  gg_theme + 
  theme(legend.position="right")

ggsave("plots/model_predictions_headedness_diff.png", plot=model.headedness.diff.g, device="png", dpi = 300, width=10)
model.headedness.diff.g

```

```{r arrange_graphs_headent_diff}

gg.headedness_entropy.arranged.diff <- ggarrange(model.entropy.diff.g, model.headedness.diff.g, nrow = 2)
ggsave("plots/model_predictions_headedness_entropy_diff.png", plot=gg.headedness_entropy.arranged, device="png", dpi = 300, width=9, height=9)
gg.headedness_entropy.arranged.diff

```

#### Headedness with entropy

```{r merge_and_diff_headANDent}

df.predict.headANDent.diff.spoken <- df.predict.headANDent %>% 
  ungroup() %>%
  filter(modality == "Spoken\n(YouTube)") %>% 
  select(-modality) %>% 
  rename(difference_predicted_spoken = difference_predicted) %>%
  mutate(entropy_scaled = round(entropy_scaled, 2))
  
df.predict.headANDent.diff.written <- df.predict.headANDent  %>% 
  ungroup() %>%
  filter(modality == "Written (UD)") %>%
  select(-modality) %>% 
  rename(difference_predicted_written = difference_predicted) %>%
  mutate(entropy_scaled = round(entropy_scaled, 2))

df.predict.headANDent.diff <- merge(df.predict.headANDent.diff.spoken, df.predict.headANDent.diff.written) %>%
  mutate(difference_score = difference_predicted_spoken - difference_predicted_written)

## Observed data

df.observed.headANDent.diff.spoken <- df.observed.headANDent %>% 
  ungroup() %>%
  filter(modality == "Spoken\n(YouTube)") %>% 
  select(-modality) %>% 
  rename(difference_score_spoken = difference_score) %>%
  mutate(entropy_scaled = round(entropy_scaled, 2))
  
df.observed.headANDent.diff.written <- df.observed.headANDent  %>% 
  ungroup() %>%
  filter(modality == "Written (UD)") %>%
  select(-modality) %>% 
  rename(difference_score_written = difference_score, lengthCat_written=lengthCat, headednessCat_written=headednessCat)  %>%
  mutate(entropy_scaled = round(entropy_scaled, 2))

df.observed.headANDent.diff <- merge(df.observed.headANDent.diff.spoken, df.observed.headANDent.diff.written) %>%
  mutate(difference_score = difference_score_spoken - difference_score_written)

```

```{r graph_headedness_entropy_diff_mean}

# This one is weird because the predicted entropy values go way beyond the ones that are shared by writing/speaking
# But the only way to get "observed" difference scores is by subtracting cases where writing and speech have the same entropy value
# It's also misleading because it looks like there are so few values
# Trying to show the "observed" differences might just not really work here? Maybe that's okay...

df.predict.headANDent.diff$lengthCat      <- factor(df.predict.headANDent.diff$lengthCat, 
                                               levels=c("Min length", "Mean length", "Max length"))
df.predict.headANDent.diff$headednessCat  <- factor(df.predict.headANDent.diff$headednessCat, 
                                               levels=c("Minimum", "Mean", "Maximum"))

df.observed.headANDent.diff$lengthCat     <- factor(df.observed.headANDent.diff$lengthCat, 
                                               levels=c("Min length", "Mean length", "Max length"))
df.observed.headANDent.diff$headednessCat <- factor(df.observed.headANDent.diff$headednessCat, 
                                               levels=c("Minimum", "Mean", "Maximum"))


model.headANDent.diff.g <- ggplot(df.predict.headANDent.diff,
                          aes(x=entropy_scaled, y=difference_score))

# Observed data
model.headANDent.diff.g <- model.headANDent.diff.g +
  geom_point(data=df.observed.headANDent.diff %>%
               group_by(entropy_scaled, headednessCat, lengthCat) %>%
               summarize(difference_score = mean(difference_score)),
             inherit.aes=FALSE,
             aes(x=(entropy_scaled), y=(difference_score),
                 color=headednessCat), shape="triangle", alpha=0.5, size=1.5)

# Prediction line plot
model.headANDent.diff.g <- model.headANDent.diff.g +
  geom_line(aes(x=(entropy_scaled), y=(difference_score), 
                color=headednessCat), linetype="dashed", alpha=0.9, size=1.5)

# Theme
model.headANDent.diff.g <- model.headANDent.diff.g +
  scale_color_manual(values=c("#61c75d", "#0086d6", "#001C49", "#61c75d")) +
  geom_hline(aes(yintercept=0), linetype="dashed", width=1.5) + 
  facet_grid(. ~ lengthCat) +
  labs(title="", x="Entropy", y="Written - Spoken", color="Head-finality") + 
  gg_theme

ggsave("plots/model_predictions_headednessEntropy_diff.png", plot=model.headANDent.diff.g, device="png", dpi = 300, width=8)
model.headANDent.diff.g

```

