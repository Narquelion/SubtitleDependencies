---
title: "YouDePP/UD - By-sentence features"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
---

# Processing

## Set-up

```{r setup, include=F, echo=F, eval=T}

knitr::opts_chunk$set(echo = TRUE)

library(dplyr)

# Package names
packages <- c("ggplot2", "ggeffects", "ghibli", "lavaan", "psych", "lme4", "lmerTest", "tidyr", "GPArotation")

# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

# Packages loading
lapply(packages, library, character.only = TRUE) %>%
  invisible()

Sys.setlocale(category = "LC_CTYPE", locale = "C")

```

### Functions

```{r utilities, include=F, echo=F, eval=T}

# Is it better to sample with replacement, or to only take *up to* x * prop samples per length?
sample_corpus <- function(df, num_samples, transitive_only=T) {
  
  # Sample x * prop observed sentences per sentence length and language
  df.sample.observed <- df %>% 
    filter(!grepl("Auto", corpus) & baseline=="observed" & (is_transitive==1 | transitive_only==F)) %>%
    mutate(rand_pos=0) %>% 
    group_by(language, corpus) %>% mutate(num_sentences=n()) %>%
    group_by(language, corpus, numDeps_noFunc) %>% mutate(prop_length=n()/num_sentences) %>% 
    sample_n(min(round(num_samples * prop_length), n()), replace=F) %>% ungroup() %>% 
    group_by(id) %>% mutate(sample_id=1:n(), reps=0, rand_pos=-1) %>% ungroup() %>%
    select(-num_sentences, -prop_length)
  
  # Get the number of times each id is repeated
  df.id_counts <- df.sample.observed %>% group_by(id) %>% summarize(reps=n())
  
  # Filer the random baseline for our selected ids and rep the random sentences x times
  df.sample.random <- df %>%
    filter(baseline=="random" & id %in% df.id_counts$id) %>% # Use only sentences in our observed sample!
    group_by(baseline, id) %>% mutate(rand_pos=ifelse(baseline=="random", 1:n(), 0)) %>% ungroup() %>%
    merge(df.id_counts) %>% group_by(id) %>%
    slice(rep(1:n(), each = head(reps, 1))) %>%
    ungroup() %>%
    mutate(sample_id=1:n(), sample_id=sample_id%%reps+1) %>% ungroup()
  
  # Do the same for the optimal data
  df.sample.optimal <- df %>%
    filter(baseline=="optimal" & id %in% df.id_counts$id) %>%
    merge(df.id_counts) %>% group_by(id) %>%
    slice(rep(1:n(), each = head(reps, 1))) %>%
    ungroup() %>% mutate(rand_pos=0) %>% 
    group_by(id) %>% mutate(sample_id=1:n(), rand_pos=-1) %>% ungroup()
  
  # Re-combine the baselines
  return(bind_rows(list(df.sample.observed, df.sample.random, df.sample.optimal)) %>% select(-sample_id, -reps))
}

# Calculate entropy as well as average DL and headedness values per sentence length

# 1. Take the average of the dependency length measurements
# 2. Calculate the proportions of different orders
# 3. Calculate the entropy of transitive sentences
# 4. Calculate mean head finality

average_diff <- function(df) {
  return(df %>% group_by(language, corpus, num_deps) %>% summarize(
    
     totalDLDiff=mean(totalDLDiff),          # Average dependency data
     averageDL=mean(averageDLDiff), 
     averageDLSLDiff=mean(averageDLSLDiff),
     
     transitive=sum(is_transitive)/n(),     # Should be all sentences in this case
     
     sov=sum(is_sov)/sum(is_transitive),    # Calculate proportion of each order; should sum to 1
     osv=sum(is_osv)/sum(is_transitive), 
     vso=sum(is_vso)/sum(is_transitive), 
     vos=sum(is_vos)/sum(is_transitive), 
     svo=sum(is_svo)/sum(is_transitive), 
     ovs=sum(is_ovs)/sum(is_transitive),
     
     entropyTrans=(-1 * ((sov * log2(max(sov, 0.000000001))) +  # The 0.00001 is a placeholder
                       (osv * log2(max(osv, 0.000000001))) + 
                       (vso * log2(max(vso, 0.000000001))) + 
                       (vos * log2(max(vos, 0.000000001))) + 
                       (svo * log2(max(svo, 0.000000001))) + 
                       (ovs * log2(max(ovs, 0.000000001))))),
  
     head_finality=mean(head_finality)) %>% ungroup() # Average headedness
  )
}
  
gg_theme <- theme_bw() + 
  theme(
    text = element_text(size = 20),
    legend.text = element_text(size = 20),
    legend.title = element_text(size = 20),
    legend.position = "bottom",
    axis.ticks = element_line(colour = "grey70", size = 0.2),
    panel.grid.major = element_line(colour = "grey70", size = 0.2),
    panel.grid.minor = element_blank()
  )

```

```{r load_data, include=F, echo=F, eval=T}

Sys.setlocale(category = "LC_CTYPE", locale = "C")
load(file='~/Documents/research/git-projects/YouDePP/data/rdata/youdepp_stats_clean.Rda')
load(file='~/Documents/research/git-projects/YouDePP/data/rdata/ud_stats_clean.Rda')

df.youdepp.stats <- df.youdepp.stats %>% 
  filter(is_transitive == 1) %>% 
  select(-is_intransitive, -is_subjdrop, -is_vonly, -is_sv, -is_vs, -is_ov, -is_vo)
df.ud.stats <- df.ud.stats %>% 
  filter(is_transitive == 1) %>% 
  select(-is_intransitive, -is_subjdrop, -is_vonly, -is_sv, -is_vs, -is_ov, -is_vo)

```

### Randomly sample data (legacy code---no longer doing this)

Transitive (that is, \{S, O, V\}) sentences are randomly sampled without replacement to create a more manageable and balanced data set. The number of samples taken at each sentence length for each corpus/channel for each language is the minimum of *either* 300/6000 (YouDePP/UD) * the proportion of sentences of a given length to the total number of sentences, *or* the total number of sentences of a given length. Sentence lengths are grouped by the number of *open-class dependencies* in the sentence. Automatically-transcribed speech data is excluded. Two different sampling factors were chosen in order to account for differences in the overall distribution of sentences in the UD and YouDePP corpora, as well as to account for the fact that each language in the YouDePP sample is divided across multiple "channels", while there is only one corpus per language in the UD sample. These sampling factors resulted in a fairly balanced number of sentences across the two data sets (YouDePP n=7455, UD n=7289).

```{r sample_data, include=T, echo=F, eval=T, message=F}

# Sample YouDePP and UD corpora
# Using different values corrects for the differences in distribution (?)
df.youdepp.stats.sampled <- sample_corpus(df.youdepp.stats, 250)
df.ud.stats.sampled      <- sample_corpus(df.ud.stats, 6000)

# Density of samples
ggplot(df.youdepp.stats.sampled, aes(x=numDeps_noFunc)) +
  geom_density() + facet_wrap(.~language) + gg_theme
ggplot(df.ud.stats.sampled, aes(x=numDeps_noFunc)) +
  geom_density() + facet_wrap(.~language) + gg_theme

df.youdepp.stats.sampled %>% filter(baseline=="observed") %>% summarize(count=n())
df.ud.stats.sampled %>% filter(baseline=="observed") %>% summarize(count=n())

# Combine datasets
#df.all.stats.sampled <- bind_rows(list("spoken"=df.youdepp.stats.sampled, "written"=df.ud.stats.sampled), .id="modality")

```

### Calculate difference scores

Difference scores are calculated as the observed dependency lengths of the sentence minus the optimal dependency lengths (for all three of: total DL, average DL, and average DL/sentence length), considering open-class words only. The function `difference_score()` only returns values relevant for looking at entropy and headedness:

```{r calculate_difference_scores}

difference_score_noFunc <- function(df) {
  
  df.ran <- df %>% filter(baseline=="random") %>% 
    select(id, totalDL_noFunc, averageDL_noFunc, averageDLSL_noFunc) %>%
    rename(totalDL_noFunc_ran=totalDL_noFunc, averageDL_noFunc_ran=averageDL_noFunc, averageDLSL_noFunc_ran=averageDLSL_noFunc)
  df.obs <- df %>% filter(baseline=="observed")
  
  df.diff <- merge(df.ran, df.obs, by="id")
  
  return(df.diff %>% 
           mutate(totalDLDiff=(totalDL_noFunc_ran-totalDL_noFunc), 
                  averageDLDiff=(averageDL_noFunc_ran-averageDL_noFunc), 
                  averageDLSLDiff=(averageDLSL_noFunc_ran-averageDLSL_noFunc)) %>%
           select(language, corpus, id, numDeps_noFunc, headFinality_noFunc,
                  is_transitive, is_sov, is_osv, is_vso, is_vos, is_svo, is_ovs, 
                  totalDLDiff, averageDLDiff, averageDLSLDiff) %>%
           rename(num_deps=numDeps_noFunc, head_finality=headFinality_noFunc))
}

difference_score_all <- function(df) {
  
  df.ran <- df %>% filter(baseline=="random") %>% 
    select(id, totalDL_all, averageDL_all, averageDLSL_all) %>%
    rename(totalDL_all_ran=totalDL_all, averageDL_all_ran=averageDL_all, averageDLSL_all_ran=averageDLSL_all)
  df.obs <- df %>% filter(baseline=="observed")
  
  df.diff <- merge(df.ran, df.obs, by="id")
  
  return(df.diff %>% 
           mutate(totalDLDiff=(totalDL_all_ran-totalDL_all), 
                  averageDLDiff=(averageDL_all_ran-averageDL_all), 
                  averageDLSLDiff=(averageDLSL_all_ran-averageDLSL_all)) %>%
           select(language, corpus, id, numDeps_all, headFinality_all,
                  is_transitive, is_sov, is_osv, is_vso, is_vos, is_svo, is_ovs, 
                  totalDLDiff, averageDLDiff, averageDLSLDiff)  %>%
           rename(num_deps=numDeps_all, head_finality=headFinality_all))
}

# Sanity check
#df.youdepp.stats.diff <- difference_score_all(df.youdepp.stats)
#df.ud.stats.diff      <- difference_score_all(df.ud.stats)

# NOTE: Currently not actually randomly sampled!
df.youdepp.stats.sampled.diff <- difference_score_noFunc(df.youdepp.stats)
df.ud.stats.sampled.diff      <- difference_score_noFunc(df.ud.stats)

```

### Calculate entropy

```{r setup_entropy, echo=F, include=F, eval=T}

# Get entropy per sentence length and channel (where available) + average values
df.youdepp.stats.sampled.diff.avg <- df.youdepp.stats.sampled.diff %>% average_diff
df.ud.stats.sampled.diff.avg      <- df.ud.stats.sampled.diff      %>% average_diff

# Add entropy to by-sentence data
df.youdepp.stats.sampled.diff.ent <- merge(df.youdepp.stats.sampled.diff, 
                                           df.youdepp.stats.sampled.diff.avg %>% 
                                             select(language, corpus, num_deps, entropyTrans)) %>% 
  filter_all(all_vars(!is.na(.) & !is.infinite(.) & !is.nan(.)))

df.ud.stats.sampled.diff.ent      <- merge(df.ud.stats.sampled.diff, 
                                           df.ud.stats.sampled.diff.avg %>% 
                                             select(language, corpus, num_deps, entropyTrans)) %>% 
  filter_all(all_vars(!is.na(.) & !is.infinite(.) & !is.nan(.)))

```

# Difference score graphs

It seems like, in most languages, there's not a huge difference between modalities when we look at this this narrower sample of sentences (i.e., transitive sentences for which headedness could be calculated) and dependency types (open-class only).

```{r graph_setup}

df.all.sampled.diff.ent <- bind_rows(list("spoken"=df.youdepp.stats.sampled.diff.ent, "written"=df.ud.stats.sampled.diff.ent), .id="modality") %>%
  filter_all(all_vars(!is.na(.) & !is.infinite(.) & !is.nan(.))) %>%
  group_by(language, corpus, modality, id) %>%
  summarize(num_deps = mean(num_deps), totalDLDiff = mean(totalDLDiff), head_finality=mean(head_finality), entropyTrans)

```

```{r difference_graphs_youdepp_narrow, eval=TRUE,  echo=FALSE, include=TRUE}

gg.youdepp.dependencies.diff <- ggplot(df.all.sampled.diff.ent, aes(x=num_deps, y=totalDLDiff)) + 
    stat_bin_hex(data=df.all.sampled.diff.ent,
                 inherit.aes=FALSE,  aes(x=num_deps, y=totalDLDiff, alpha=(1/10)*log(..density..)), 
                 bins=50, binwidth = c(1,2),
                 show.legend=FALSE, fill="black") +
    geom_smooth(aes(color=language), se = TRUE, method = "lm", formula = y ~ x) + 
    scale_fill_gradientn(colours=c("#f8f9f5","#000000")) +
    labs(title="", x="Sentence Length", y="Random-Observed", color="") + 
    theme(legend.position="bottom") +
    facet_grid(modality~language) + 
    gg_theme

#ggsave("plots/youdepp_dependencies_diff.png", plot=gg.youdepp.dependencies.diff, device="png", dpi = 300, width=10)
gg.youdepp.dependencies.diff

```

# Headedness

### Overall distributions

```{r graphs_overall_headedness}

gg.violin.overall.headedness <- ggplot(df.all.sampled.diff.ent, aes(x=language, y=head_finality)) + 
    geom_violin(aes(color=language), show.legend=F) + 
    labs(title="", x="Language", y="Head-finality", color="") + 
    theme(legend.position="bottom") +
    facet_grid(modality~language) + 
    gg_theme

#ggsave("plots/total_dependencies_all_languages.png", plot=all.dependencies, device="png", dpi = 300, width=10)
gg.violin.overall.headedness

```

### Trends by sentence length

```{r graphs_all_headedness}

gg.all.headedness <- ggplot(df.all.sampled.diff.ent, aes(x=num_deps, y=head_finality)) + 
    geom_point(aes(color=language), show.legend=F) + 
    geom_smooth(method="lm", formula=y~x, color="#666666") + 
    labs(title="", x="Sentence Length", y="Headedness", color="") + 
    theme(legend.position="bottom") +
    facet_grid(modality~language) + 
    gg_theme

#ggsave("plots/total_dependencies_all_languages.png", plot=all.dependencies, device="png", dpi = 300, width=10)
gg.all.headedness

```



# Entropy

## Distributions (average enrtopy at each sentence length)

JA, EN, and KO have consistently low entropy; FR, RU, IT, and TR are more variable. Big difference between written and spoken French.

```{r graphs_entropy_trans}

gg.violin.overall.entropy.trans <- ggplot(df.all.sampled.diff.ent, aes(x=language, y=entropyTrans)) + 
    geom_violin(aes(color=language, fill=language), alpha=0.5, show.legend=F) + 
    labs(title="", x="Language", y="Entropy of S, V, O", color="") + 
    theme(legend.position="bottom") +
    facet_wrap(.~modality, ncol=1) + 
    scale_color_ghibli_d("MononokeMedium") +
    scale_fill_ghibli_d("MononokeMedium") +
    gg_theme

ggsave("plots/entropy_distributions_trans.png", plot=gg.violin.overall.entropy.trans, device="png", dpi = 300, width=10)
gg.violin.overall.entropy.trans

```

## Entropy LMs

```{r entropy_regression}

df.entropy.lm <- df.all.sampled.diff.ent %>% 
  group_by(language, num_deps, corpus, modality, id) %>% 
  summarize(entropyTrans=mean(entropyTrans)) %>%
  mutate(baslineWritten=ifelse(modality=="written", 0, 1))

lm.entropy.fr <- lmer(entropyTrans ~ num_deps * baslineWritten + (1 | corpus), data=df.entropy.lm %>% filter(language=="fr"))
lm.entropy.ru <- lmer(entropyTrans ~ num_deps * baslineWritten + (1 | corpus), data=df.entropy.lm %>% filter(language=="ru"))
lm.entropy.en <- lmer(entropyTrans ~ num_deps * baslineWritten + (1 | corpus), data=df.entropy.lm %>% filter(language=="en"))
lm.entropy.ja <- lmer(entropyTrans ~ num_deps * baslineWritten + (1 | corpus), data=df.entropy.lm %>% filter(language=="ja"))
lm.entropy.ko <- lmer(entropyTrans ~ num_deps * baslineWritten + (1 | corpus), data=df.entropy.lm %>% filter(language=="ko"))
lm.entropy.it <- lmer(entropyTrans ~ num_deps * baslineWritten + (1 | corpus), data=df.entropy.lm %>% filter(language=="it"))
lm.entropy.tr <- lmer(entropyTrans ~ num_deps * baslineWritten + (1 | corpus), data=df.entropy.lm %>% filter(language=="tr"))

summary(lm.entropy.fr)
summary(lm.entropy.ru)
summary(lm.entropy.en)
summary(lm.entropy.ja)
summary(lm.entropy.ko)
summary(lm.entropy.tr)

```


## Entropy by sentence length

Entropy generally decreases as sentence length increases, with Korean and Japanese as exceptions.

```{r graphs_youdepp_entropy_svo}

gg.youdepp.entropy.trans <- ggplot(df.all.sampled.diff.ent %>% group_by(modality, language, corpus, num_deps) %>% summarize(entropyTrans=mean(entropyTrans)), aes(x=num_deps, y=entropyTrans)) + 
    geom_point(alpha=0.25, show.legend=F) + 
    geom_smooth(aes(color=language), method="lm", formula=y~x) + 
    labs(title="", x="Sentence Length", y="Entropy of S, V, O", color="") + 
    theme(legend.position="bottom") +
    facet_grid(modality~language) +
    #scale_color_ghibli_d("MononokeMedium") +
    #scale_fill_ghibli_d("MononokeMedium") +
    gg_theme

#ggsave("plots/entropy_by_length.png", plot=gg.youdepp.entropy.trans, device="png", dpi = 300, width=10)
gg.youdepp.entropy.trans

```

# Linear Models

## YouDePP

```{r lmer_by_sentence_youdepp} 

df.youdepp.lm.bysent.diff <- df.youdepp.stats.sampled.diff.ent %>% 
  select(language, corpus, id, num_deps, totalDLDiff, averageDLDiff, averageDLSLDiff, head_finality, entropyTrans) %>% 
  mutate(language=factor(language), corpus=factor(corpus), id=factor(id), s_len_sq=num_deps^2)

lm.features.youdep.bysent.diff <- lmer(totalDLDiff ~ 
                                         scale(s_len_sq) + scale(head_finality) + scale(entropyTrans) + 
                                         scale(entropyTrans):scale(head_finality) + 
                                         scale(s_len_sq):scale(entropyTrans) + 
                                         (1 | id) + (1 | language) + (1 | corpus),# + 
                                         #(1 + language | entropyTrans)  + (1 + corpus | entropyTrans) + 
                                         #(1 + language | head_finality) + (1 + corpus | head_finality),
                                       data=df.youdepp.lm.bysent.diff,
                                       control=lmerControl(optimizer="bobyqa"))

summary(lm.features.youdep.bysent.diff)

# How do predictions change with higher entropy + longer sentences? Priors are different...do you KNOW you're in a longer sentence? (E.g. news vs. writing vs. comedy...)

```

### UD

```{r lmer_by_sentence_ud} 

df.ud.lm.bysent.diff <- df.ud.stats.sampled.diff.ent %>% 
  select(language, corpus, id, num_deps, totalDLDiff, averageDLDiff, averageDLSLDiff, head_finality, entropyTrans) %>% 
  mutate(language=factor(language), corpus=factor(corpus), id=factor(id), s_len_sq=num_deps^2)

lm.features.youdep.bysent.diff <- lmer(totalDLDiff ~ 
                                         scale(s_len_sq) + scale(head_finality) + scale(entropyTrans) + 
                                         scale(entropyTrans):scale(head_finality) + 
                                         scale(s_len_sq):scale(entropyTrans) + 
                                         (1 | id) + (1 | language), data=df.ud.lm.bysent.diff,
                              control=lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=100000)))

summary(lm.features.youdep.bysent.diff)

```

## Written & spoken

### Legacy setup (no longer sampling!)

```{r cross_modal_setup}

# Sample wth replacement multiple times & take average at each language x corpus x length  combination
# 100 averages

languages <- c("en", "fr", "it", "tr", "ja", "ko", "ru")
df.all.diff.sampled <- data.frame()

# Suppress summarise info
options(dplyr.summarise.inform = FALSE)

for (i in languages) {
  
  df.youdepp.temp.l <- df.youdepp.stats.sampled.diff.ent %>% filter(language==i)
  df.ud.temp.l      <- df.ud.stats.sampled.diff.ent %>% filter(language==i)
  
  df.youdepp.temp.l <- df.youdepp.temp.l %>% filter(num_deps %in% lengths)
  df.ud.temp.l      <- df.ud.temp.l %>% filter(num_deps %in% lengths)
  
  lengths <- intersect(unique(df.youdepp.temp.l$num_deps), unique(df.ud.temp.l$num_deps))
  
  for(j in lengths) {
  
    df.youdepp.temp.l.n <- df.youdepp.temp.l %>% filter(num_deps==j)
    df.ud.temp.l.n      <- df.ud.temp.l %>% filter(num_deps==j)
  
    total_n  <- nrow(df.youdepp.temp.l) + nrow(df.ud.temp.l)
    subset_n <- nrow(df.youdepp.temp.l.n) + nrow(df.ud.temp.l.n)
    prop_l_n <- subset_n/total_n
    
    for (k in 1:round(10000 * prop_l_n)) {
      
      df.youdepp.temp.sampled <- df.youdepp.temp.l.n %>% 
        group_by(language, num_deps) %>% 
        sample_n(n(), replace=T) %>%
        summarize(totalDLDiff_s = mean(totalDLDiff), 
                  averageDLDiff_s = mean(averageDLDiff), 
                  averageDLSLDiff_s = mean(averageDLSLDiff), 
                  head_finality_s = mean(head_finality), 
                  entropyTrans_s = mean(entropyTrans)) %>% ungroup()
      
      df.ud.temp.sampled <- df.ud.temp.l.n %>% 
        group_by(language, num_deps) %>% 
        sample_n(n(), replace=T) %>%
        summarize(totalDLDiff_w = mean(totalDLDiff), 
                  averageDLDiff_w = mean(averageDLDiff), 
                  averageDLSLDiff_w = mean(averageDLSLDiff), 
                  head_finality_w = mean(head_finality), 
                  entropyTrans_w = mean(entropyTrans)) %>% ungroup()
      
      # Written - spoken
      df.all.diff <- merge(df.youdepp.temp.sampled, df.ud.temp.sampled, by=c("language", "num_deps")) %>%
        mutate(totalDLDiff=(totalDLDiff_w-totalDLDiff_s), 
               averageDLDiff=(averageDLDiff_w-averageDLDiff_s), 
               averageDLSLDiff=(averageDLSLDiff_w-averageDLSLDiff_s), 
               head_finality=(head_finality_w-head_finality_s), 
               entropyTrans=sum(entropyTrans_w-entropyTrans_s)) %>%
        select(language, num_deps, totalDLDiff, averageDLDiff, averageDLSLDiff, head_finality, entropyTrans)
      
      df.all.diff.sampled <- bind_rows(df.all.diff.sampled, df.all.diff)
    }
  }
}

df.all.diff.sampled %>% group_by(language) %>% summarize(count=n(), avgDiffTot=mean(totalDLDiff), avgDiffHead=mean(head_finality), avgDiffEnt=mean(entropyTrans))

ggplot(df.all.diff.sampled, aes(x=factor(num_deps), y=totalDLDiff)) +
  geom_violin() + 
  facet_wrap(.~language)

```

```{r written_spoken_regression_sampled}

df.all.lm.bysent.diff <- df.all.diff.sampled %>% 
  select(language, num_deps, totalDLDiff, averageDLDiff, averageDLSLDiff, head_finality, entropyTrans) %>% 
  mutate(language=factor(language), s_len_sq=num_deps^2)

lm.features.all.bysent.diff <- lmer(totalDLDiff ~ 
                                         scale(s_len_sq) + scale(head_finality) + scale(entropyTrans) + 
                                         scale(entropyTrans):scale(head_finality) + 
                                         scale(s_len_sq):scale(entropyTrans) + 
                                         (1 | language), data=df.all.lm.bysent.diff,
                              control=lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=100000)))

summary(lm.features.all.bysent.diff)

#(rand_w - obs_w) - (rand_s - obs_s)
#rand - obs bigger = more minimization within modality
# diff_w - diff_s bigger = more minimization IN WRITING

```


### Full regression setup

```{r regression_full_setup}

df.all.stats.diff <- bind_rows(list("spoken"=df.youdepp.stats.sampled.diff.ent, "written"=df.ud.stats.sampled.diff.ent), .id="modality")

# Clean up data for regression
df.all.lm <- df.all.stats.diff %>% 
  mutate(spoken            = ifelse(modality=="written", 0, 1), 
         written           = ifelse(modality=="written", 1, 0), 
         s_len_sq          = num_deps^2) %>%
  rename(difference_score = totalDLDiff) %>%
  select(modality, language, corpus, id, num_deps,
         s_len_sq, difference_score, head_finality, entropyTrans)

# Take the average difference of each sentence for easier regression calculation
# (Doesn't change results, but should maybe use full data later?)
df.all.avg.lm <- df.all.lm %>% 
  group_by(id) %>% 
  summarize(language         = head(language, 1),
            corpus           = head(corpus, 1),
            modality         = head(modality, 1),
            num_deps         = head(num_deps, 1),
            head_finality    = mean(head_finality), 
            entropyTrans     = mean(entropyTrans), 
            difference_score = mean(difference_score)) %>% 
  ungroup() %>%
  mutate(s_len_sq = num_deps^2,
         spoken   = ifelse(modality=="written", 0, 1), 
         written  = ifelse(modality=="written", 1, 0),
         s_len_sq_scaled   = scale(s_len_sq),
         headedness_scaled = scale(head_finality),
         entropy_scaled    = scale(entropyTrans))

```

### Baseline written

```{r regression_full_baselineWritten}

lm.diff.avg.baseWritten <- lmer(difference_score ~ 
      # Main effects
      spoken + s_len_sq_scaled + headedness_scaled + entropy_scaled + 
      # Main interactions
      headedness_scaled:entropy_scaled + 
      s_len_sq_scaled:entropy_scaled + 
      #s_len_sq_scaled:headedness_scaled + 
      # 3rd order
      #s_len_sq_scaled:headedness_scaled:entropy_scaled + 
      # Interactions with baseline
      spoken:s_len_sq_scaled + 
      spoken:headedness_scaled + spoken:entropy_scaled + 
      # 3rd order interactions
      spoken:headedness_scaled:entropy_scaled + 
      #spoken:s_len_sq_scaled:headedness_scaled + 
      spoken:s_len_sq_scaled:entropy_scaled + 
      # 4th order?????
      #spoken:s_len_sq_scaled:entropy_scaled:headedness_scaled + 
      #(1 | id) + 
      (1 | language) + (1 | corpus), data=df.all.avg.lm,
      control=lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=100000)))


conf.lm.diff.avg.baseWritten <- confint(lm.diff.avg.baseWritten, parm="beta_", method="Wald")
conf.lm.diff.avg.baseWritten

summary(lm.diff.avg.baseWritten)
#summary(lm.features.diff.full.averaged.baselineWritten)

```

### Baseline spoken

```{r regression_full_baselineSpoken}

lm.diff.avg.baseSpoken <- lmer(difference_score ~ 
      # Main effects
      written + s_len_sq_scaled + headedness_scaled + entropy_scaled + 
      # Main interactions
      headedness_scaled:entropy_scaled + 
      s_len_sq_scaled:entropy_scaled + 
      s_len_sq_scaled:headedness_scaled + 
      # 3rd order
      s_len_sq_scaled:headedness_scaled:entropy_scaled + 
      # Interactions with baseline
      written:s_len_sq_scaled + 
      written:headedness_scaled + written:entropy_scaled + 
      # 3rd order interactions
      written:headedness_scaled:entropy_scaled + 
      written:s_len_sq_scaled:headedness_scaled + 
      written:s_len_sq_scaled:entropy_scaled + 
      # 4th order?????
      written:s_len_sq_scaled:entropy_scaled:headedness_scaled + 
      #(1 | id) + 
      (1 | language) + (1 | corpus), data=df.all.avg.lm,
      control=lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=100000)))

summary(lm.diff.avg.baseSpoken)
conf.lm.diff.avg.baseSpoken <- confint(lm.diff.avg.baseSpoken, parm="beta_", method="Wald")
conf.lm.diff.avg.baseSpoken

```

## Model predictions

### Setup

```{r set_up_predictions_setup}

# Separate data sets for within-baseline calculations
df.all.avg.lm.spoken  <- df.all.avg.lm %>% filter(spoken == 1)
df.all.avg.lm.written <- df.all.avg.lm %>% filter(spoken == 0)

# Calculate useful variables for expanding
hf_min.spoken  = min(df.all.avg.lm.spoken$headedness_scaled)
hf_max.spoken  = max(df.all.avg.lm.spoken$headedness_scaled)
hf_mean.spoken = mean(df.all.avg.lm.spoken$headedness_scaled)
hf_sd.spoken   = sd(df.all.avg.lm.spoken$headedness_scaled)

ent_min.spoken  = min(df.all.avg.lm.spoken$entropy_scaled)
ent_max.spoken  = max(df.all.avg.lm.spoken$entropy_scaled)
ent_mean.spoken = mean(df.all.avg.lm.spoken$entropy_scaled)
ent_sd.spoken   = sd(df.all.avg.lm.spoken$entropy_scaled)

len_min.spoken  <- min(df.all.avg.lm.spoken$s_len_sq_scaled)
len_mean.spoken <- mean(df.all.avg.lm.spoken$s_len_sq_scaled)
len_max.spoken  <- max(df.all.avg.lm.spoken$s_len_sq_scaled)
len_sd.spoken   <- sd(df.all.avg.lm.spoken$s_len_sq_scaled)

hf_min.written  = min(df.all.avg.lm.written$headedness_scaled)
hf_max.written  = max(df.all.avg.lm.written$headedness_scaled)
hf_mean.written = mean(df.all.avg.lm.written$headedness_scaled)
hf_sd.written   = sd(df.all.avg.lm.written$headedness_scaled)

ent_min.written  = min(df.all.avg.lm.written$entropy_scaled)
ent_max.written  = max(df.all.avg.lm.written$entropy_scaled)
ent_mean.written = mean(df.all.avg.lm.written$entropy_scaled)
ent_sd.written   = sd(df.all.avg.lm.written$entropy_scaled)

len_min.written  <- min(df.all.avg.lm.written$s_len_sq_scaled)
len_mean.written <- mean(df.all.avg.lm.written$s_len_sq_scaled)
len_max.written  <- max(df.all.avg.lm.written$s_len_sq_scaled)
len_sd.written   <- sd(df.all.avg.lm.written$s_len_sq_scaled)

```

### Headedness vs. entropy

```{r set_up_predictions_headednessVsEntropy}

# Spoken prediction data
df.predict.headVent.spoken <- expand.grid(
  headedness_scaled = c(hf_min.spoken, hf_mean.spoken, hf_max.spoken), 
  s_len_sq_scaled   = seq(1, 15, by=1),
  entropy_scaled    = c(ent_min.spoken, ent_mean.spoken, ent_max.spoken), 
  spoken            = c(1),
  language          = unique(df.all.avg.lm.spoken$language),  
  corpus            = unique(df.all.avg.lm.spoken$corpus)
) %>%
  mutate(s_len_sq_scaled = s_len_sq_scaled^2, 
         s_len_sq_scaled = scale(s_len_sq_scaled))

# Written prediction data
df.predict.headVent.written <- expand.grid(
  headedness_scaled = c(hf_min.written, hf_mean.written, hf_max.written), 
  s_len_sq_scaled   = seq(1, 15, by=1),
  entropy_scaled    = c(ent_min.written, ent_mean.written, ent_max.written), 
  spoken            = c(0), 
  language          = unique(df.all.avg.lm.written$language),  
  corpus            = unique(df.all.avg.lm.written$corpus)
) %>%
  mutate(s_len_sq_scaled = s_len_sq_scaled^2, 
         s_len_sq_scaled = scale(s_len_sq_scaled))

# Predict spoken
df.predict.headVent.spoken$difference_predicted <- predict(lm.diff.avg.baseWritten, newdata=df.predict.headVent.spoken)

# Predict written
df.predict.headVent.written$difference_predicted <- predict(lm.diff.avg.baseWritten, newdata=df.predict.headVent.written)

```

```{r clean_predictions_headednessVsEntropy}

# Give our entropy and headedness categories nice names
df.predict.headVent.spoken <- df.predict.headVent.spoken %>% 
  mutate(headednessCat = case_when(headedness_scaled == hf_min.spoken ~ "Min head-finality", 
                                   headedness_scaled == hf_max.spoken ~ "Max head-finality", 
                                   TRUE ~ "Mean head-finality"),
         entropyCat    = case_when(entropy_scaled == ent_min.spoken ~ "Min entropy",
                                   entropy_scaled == ent_max.spoken ~ "Max entropy", 
                                   TRUE ~ "Mean entropy"),
         modality = ifelse(spoken==0, "Written (UD)", "Spoken (YouTube)"))
 
df.predict.headVent.written <- df.predict.headVent.written %>% 
  mutate(headednessCat = case_when(headedness_scaled == hf_min.written ~ "Min head-finality", 
                                   headedness_scaled == hf_max.written ~ "Max head-finality", 
                                   TRUE ~ "Mean head-finality"),
         entropyCat    = case_when(entropy_scaled == ent_min.written ~ "Min entropy",
                                   entropy_scaled == ent_max.written ~ "Max entropy", 
                                   TRUE ~ "Mean entropy"),
         modality = ifelse(spoken==0, "Written (UD)", "Spoken (YouTube)"))

# Combine dataframes
df.predict.headVent <- bind_rows(df.predict.headVent.spoken, df.predict.headVent.written)

# Collapse unnecessary predictors
df.predict.headVent <- df.predict.headVent %>% 
  group_by(s_len_sq_scaled, modality, headednessCat, entropyCat) %>% 
  summarize(difference_predicted = mean(difference_predicted))

```

```{r clean_observed_headednessVsEntropy}

# Spoken
df.observed.headVent.spoken <- df.all.avg.lm.spoken %>%
  mutate(entropyCat = case_when(entropy_scaled <= (ent_mean.spoken - ent_sd.spoken) ~ "Min entropy",
                                entropy_scaled >= (ent_mean + ent_sd.spoken) ~ "Max entropy",
                                TRUE ~ "Mean entropy"),
         headednessCat = case_when(headedness_scaled <= (ent_mean.spoken - ent_sd.spoken) ~ "Min headedness",
                                headedness_scaled >= (ent_mean + ent_sd.spoken) ~ "Max headedness",
                                TRUE ~ "Mean headedness"),
         modality = ifelse(spoken==0, "Written (UD)", "Spoken (YouTube)")) %>% 
  group_by(s_len_sq_scaled, spoken, headednessCat, entropyCat) %>% 
  summarize(difference_score = mean(difference_score))

# Written
df.observed.headVent.written <- df.all.avg.lm.written %>%
  mutate(entropyCat = case_when(entropy_scaled <= (ent_mean.written - ent_sd.written) ~ "Min entropy",
                                entropy_scaled >= (ent_mean + ent_sd.written) ~ "Max entropy",
                                TRUE ~ "Mean entropy"),
         headednessCat = case_when(headedness_scaled <= (ent_mean.written - ent_sd.written) ~ "Min headedness",
                                headedness_scaled >= (ent_mean + ent_sd.written) ~ "Max headedness",
                                TRUE ~ "Mean headedness"),
         modality = ifelse(spoken==0, "Written (UD)", "Spoken (YouTube)")) %>% 
  group_by(s_len_sq_scaled, spoken, headednessCat, entropyCat) %>% 
  summarize(difference_score = mean(difference_score))
  
df.observed.headVent <- bind_rows(df.observed.headVent.spoken, df.observed.headVent.written)

```

```{r graph_entropy}

model.entropy.g <- ggplot(df.predict.head_ent %>% 
                            group_by(modality, s_len_sq_scaled, entropyCat) %>% 
                            summarize(difference_predicted = mean(difference_predicted)), 
                          aes(x=s_len_sq_scaled, y=difference_predicted, color=entropyCat))

# Observed data
model.entropy.g <- model.entropy.g +
  geom_point(data=df.observed.head_ent,
             inherit.aes=FALSE, 
             aes(x=(s_len_sq_scaled), y=(difference_score), 
                 color=entropyCat), shape="triangle", alpha=0.5, size=1.5)
 
# Prediction line plot
model.entropy.g <- model.entropy.g +
  geom_line(aes(x=(s_len_sq_scaled), y=(difference_predicted), 
                color=entropyCat), linetype="dashed", alpha=0.75, size=1.5)

# Theme
model.entropy.g <- model.entropy.g +
  scale_color_manual(values=c("#61c75d", "#0086d6", "#001C49", "#61c75d", "#0086d6", "#001C49")) +
  geom_hline(aes(yintercept=0), linetype="dashed", width=1.5) + 
  coord_cartesian(ylim=c(-2, 15)) + 
  facet_wrap(. ~ modality) +
  labs(title="", x="Sentence Length", y="Random - Observed", color="Entropy") + 
  gg_theme() + 
  theme(legend.position="bottom")

ggsave("plots/model_predictions_entropy.png", plot=model.entropy.g, device="png", dpi = 300, width=10)
model.entropy.g

```

```{r graph_headedness}

model.headedness.g <- ggplot(df.predict.head_ent %>% 
                            group_by(modality, s_len_sq_scaled, headednessCat) %>% 
                            summarize(difference_predicted = mean(difference_predicted)), 
                          aes(x=s_len_sq_scaled, y=difference_predicted, color=headednessCat))

# Observed data
model.headedness.g <- model.headedness.g +
  geom_point(data=df.observed.head_ent,
             inherit.aes=FALSE, 
             aes(x=(s_len_sq_scaled), y=(difference_score), 
                 color=headednessCat), shape="triangle", alpha=0.5, size=1.5)
 
# Prediction line plot
model.headedness.g <- model.headedness.g +
  geom_line(aes(x=(s_len_sq_scaled), y=(difference_predicted), 
                color=headednessCat), linetype="dashed", alpha=0.75, size=1.5)

# Theme
model.headedness.g <- model.headedness.g +
  scale_color_manual(values=c("#61c75d", "#0086d6", "#001C49", "#61c75d", "#0086d6", "#001C49")) +
  geom_hline(aes(yintercept=0), linetype="dashed", width=1.5) + 
  coord_cartesian(ylim=c(-2, 15)) + 
  facet_wrap(. ~ modality) +
  labs(title="", x="Sentence Length", y="Random - Observed", color="Head-finality") + 
  gg_theme()  + 
  theme(legend.position="bottom")

ggsave("plots/model_predictions_headedness.png", plot=model.headedness.g, device="png", dpi = 300, width=10)
model.headedness.g

```

### Headedness together with entropy

```{r set_up_predictions_headednessAndEntropy}

# Spoken prediction data
df.predict.headANDent.spoken <- expand.grid(
  headedness_scaled = c(hf_min.spoken, hf_mean.spoken, hf_max.spoken), 
  s_len_sq_scaled   = c(len_min.spoken, len_mean.spoken, len_max.spoken),
  entropy_scaled    = seq(ent_min.spoken, ent_max.spoken, by=0.1), 
  spoken            = c(1),
  language          = unique(df.all.avg.lm.spoken$language),  
  corpus            = unique(df.all.avg.lm.spoken$corpus)
) 

# Written prediction data
df.predict.headANDent.written <- expand.grid(
  headedness_scaled = c(hf_min.written, hf_mean.written, hf_max.written), 
  s_len_sq_scaled   = c(len_min.written, len_mean.written, len_max.written),
  entropy_scaled    = seq(ent_min.written, ent_max.written, by=0.1), 
  spoken            = c(0), 
  language          = unique(df.all.avg.lm.written$language),  
  corpus            = unique(df.all.avg.lm.written$corpus)
) 

# Predict spoken
df.predict.headANDent.spoken$difference_predicted <- predict(lm.diff.avg.baseWritten, newdata=df.predict.headANDent.spoken)

# Predict written
df.predict.headANDent.written$difference_predicted <- predict(lm.diff.avg.baseWritten, newdata=df.predict.headANDent.written)

```

```{r clean_predictions_headednessAndEntropy}

# Give our entropy and headedness categories nice names
df.predict.headANDent.spoken <- df.predict.headANDent.spoken %>% 
  mutate(headednessCat = case_when(headedness_scaled == hf_min.spoken ~ "Min head-finality", 
                                   headedness_scaled == hf_max.spoken ~ "Max head-finality", 
                                   TRUE ~ "Mean head-finality"),
         lengthCat    = case_when(s_len_sq_scaled == len_min.spoken ~ "Min length",
                                   s_len_sq_scaled == len_max.spoken ~ "Max length", 
                                   TRUE ~ "Mean length"),
         modality = ifelse(spoken==0, "Written (UD)", "Spoken (YouTube)"))
 
df.predict.headANDent.written <- df.predict.headANDent.written %>% 
  mutate(headednessCat = case_when(headedness_scaled == hf_min.written ~ "Min head-finality", 
                                   headedness_scaled == hf_max.written ~ "Max head-finality", 
                                   TRUE ~ "Mean head-finality"),
         lengthCat    = case_when(s_len_sq_scaled == len_min.written ~ "Min length",
                                   s_len_sq_scaled == len_max.written ~ "Max length", 
                                   TRUE ~ "Mean length"),
         modality = ifelse(spoken==0, "Written (UD)", "Spoken (YouTube)"))

# Combine dataframes
df.predict.headANDent <- bind_rows(df.predict.headANDent.spoken, df.predict.headANDent.written)

# Collapse unnecessary predictors
df.predict.headANDent <- df.predict.headANDent %>% 
  group_by(entropy_scaled, modality, headednessCat, lengthCat) %>% 
  summarize(difference_predicted = mean(difference_predicted))

  
```

```{r clean_observed_headednessAndEntropy}

# Spoken
df.observed.headANDent.spoken <- df.all.avg.lm.spoken %>%
  mutate(lengthCat = case_when(s_len_sq_scaled <= (len_mean.spoken - len_sd.spoken) ~ "Min length",
                                s_len_sq_scaled >= (len_mean.spoken + len_sd.spoken) ~ "Max length",
                                TRUE ~ "Mean length"),
         headednessCat = case_when(headedness_scaled <= (hf_mean.spoken - hf_sd.spoken) ~ "Min head-finality",
                                headedness_scaled >= (hf_mean.spoken + hf_sd.spoken) ~ "Max head-finality",
                                TRUE ~ "Mean head-finality"),
         modality = ifelse(spoken==0, "Written (UD)", "Spoken (YouTube)")) %>% 
  group_by(entropy_scaled, modality, headednessCat, lengthCat) %>% 
  summarize(difference_score = mean(difference_score))

# Written
df.observed.headANDent.written <- df.all.avg.lm.written %>%
  mutate(lengthCat = case_when(s_len_sq_scaled <= (len_mean.written - len_sd.written) ~ "Min length",
                                s_len_sq_scaled >= (len_mean.written + len_sd.written) ~ "Max length",
                                TRUE ~ "Mean length"),
         headednessCat = case_when(headedness_scaled <= (hf_mean.written - hf_sd.written) ~ "Min head-finality",
                                headedness_scaled >= (hf_mean.written + hf_sd.written) ~ "Max head-finality",
                                TRUE ~ "Mean head-finality"),
         modality = ifelse(spoken==0, "Written (UD)", "Spoken (YouTube)")) %>% 
  group_by(entropy_scaled, modality, headednessCat, lengthCat) %>% 
  summarize(difference_score = mean(difference_score))
  
df.observed.headANDent <- bind_rows(df.observed.headANDent.spoken, df.observed.headANDent.written)

df.observed.headANDent %>% group_by(modality, lengthCat) %>% summarize(count=n())
```

```{r graph_headedness_entropy_mean}

model.headANDent.mean.g <- ggplot(df.predict.headANDent %>% filter(lengthCat == "Mean length"),
                          aes(x=entropy_scaled, y=difference_predicted))

# Observed data
model.headANDent.mean.g <- model.headANDent.mean.g +
  geom_point(data=df.observed.headANDent  %>% filter(lengthCat == "Mean length"),
             inherit.aes=FALSE,
             aes(x=(entropy_scaled), y=(difference_score),
                 color=headednessCat), shape="triangle", alpha=0.7, size=1.5)

# Prediction line plot
model.headANDent.mean.g <- model.headANDent.mean.g +
  geom_line(aes(x=(entropy_scaled), y=(difference_predicted), 
                color=headednessCat), linetype="dashed", alpha=0.9, size=1.5)

# Theme
model.headANDent.mean.g <- model.headANDent.mean.g +
  scale_color_manual(values=c("#61c75d", "#0086d6", "#001C49", "#61c75d", "#0086d6", "#001C49")) +
  #geom_hline(aes(yintercept=0), linetype="dashed", width=1.5) + 
  coord_cartesian(ylim=c(-1, 3)) + 
  facet_grid(lengthCat ~ modality) +
  labs(title="", x="Entropy", y="Random - Observed", color="Head-finality") + 
  gg_theme()  + 
  theme(legend.position="bottom")

ggsave("plots/model_predictions_headednessEntropy_mean.png", plot=model.headANDent.mean.g, device="png", dpi = 300, width=10)
model.headANDent.mean.g

```

```{r graph_headedness_entropy_max}

model.headANDent.max.g <- ggplot(df.predict.headANDent %>% filter(lengthCat == "Max length"),
                          aes(x=entropy_scaled, y=difference_predicted))

# Observed data
# model.headANDent.max.g <- model.headANDent.max.g +
#   geom_point(data=df.observed.headANDent  %>% filter(lengthCat == "Max length"),
#              inherit.aes=FALSE,
#              aes(x=(entropy_scaled), y=(difference_score),
#                  color=headednessCat), shape="triangle", alpha=0.7, size=1.5)

# Prediction line plot
model.headANDent.max.g <- model.headANDent.max.g +
  geom_line(aes(x=(entropy_scaled), y=(difference_predicted), 
                color=headednessCat), linetype="dashed", alpha=0.9, size=1.5)

# Theme
model.headANDent.max.g <- model.headANDent.max.g +
  scale_color_manual(values=c("#61c75d", "#0086d6", "#001C49", "#61c75d", "#0086d6", "#001C49")) +
  #geom_hline(aes(yintercept=0), linetype="dashed", width=1.5) + 
  #coord_cartesian(ylim=c(5, 15)) + 
  facet_grid(lengthCat ~ modality) +
  labs(title="", x="Entropy", y="Random - Observed", color="Head-finality") + 
  gg_theme()  + 
  theme(legend.position="bottom")

ggsave("plots/model_predictions_headednessEntropy_max.png", plot=model.headANDent.max.g, device="png", dpi = 300, width=10)
model.headANDent.max.g

```

### Written vs. spoken difference scores

```{r merge_and_diff_headVent}

df.predict.headVent.diff.spoken <- df.predict.headVent %>% 
  ungroup() %>%
  filter(modality == "Spoken (YouTube)") %>% 
  select(-modality) %>% 
  rename(difference_predicted_spoken = difference_predicted) %>%
  mutate(s_len_sq_scaled = round(s_len_sq_scaled, 2))
  
df.predict.headVent.diff.written <- df.predict.headVent  %>% 
  ungroup() %>%
  filter(modality == "Written (UD)") %>%
  select(-modality) %>% 
  rename(difference_predicted_written = difference_predicted) %>%
  mutate(s_len_sq_scaled = round(s_len_sq_scaled, 2))

df.predict.headVent.diff <- merge(df.predict.headVent.diff.spoken, df.predict.headVent.diff.written) %>%
  mutate(difference_score = difference_predicted_spoken - difference_predicted_written)

```

```{r graph_entropy_difference}

model.entropy.diff.g <- ggplot(df.predict.headVent.diff %>% 
                            group_by(s_len_sq_scaled, entropyCat) %>% 
                            summarize(difference_score = mean(difference_score)), 
                          aes(x=s_len_sq_scaled, y=difference_score, color=entropyCat))

# Prediction line plot
model.entropy.diff.g <- model.entropy.diff.g +
  geom_line(aes(x=(s_len_sq_scaled), y=(difference_score), 
                color=entropyCat), linetype="dashed", alpha=0.75, size=1.5)

# Theme
model.entropy.diff.g <- model.entropy.diff.g +
  scale_color_manual(values=c("#61c75d", "#0086d6", "#001C49")) +
  geom_hline(aes(yintercept=0), linetype="dashed", width=1.5) + 
  #coord_cartesian(ylim=c(-2, 15)) + 
  labs(title="", x="Sentence Length", y="Written - Spoken", color="Head-finality") + 
  gg_theme()  + 
  theme(legend.position="bottom")

ggsave("plots/model_predictions_entropy_diff.png", plot=model.entropy.diff.g, device="png", dpi = 300, width=10)
model.entropy.diff.g

```

```{r graph_headedness_difference}

model.headedness.diff.g <- ggplot(df.predict.headVent.diff %>% 
                            group_by(s_len_sq_scaled, headednessCat) %>% 
                            summarize(difference_score = mean(difference_score)), 
                          aes(x=s_len_sq_scaled, y=difference_score, color=headednessCat))

# Prediction line plot
model.headedness.diff.g <- model.headedness.diff.g +
  geom_line(aes(x=(s_len_sq_scaled), y=(difference_score), 
                color=headednessCat), linetype="dashed", alpha=0.75, size=1.5)

# Theme
model.headedness.diff.g <- model.headedness.diff.g +
  scale_color_manual(values=c("#61c75d", "#0086d6", "#001C49")) +
  geom_hline(aes(yintercept=0), linetype="dashed", width=1.5) + 
  #coord_cartesian(ylim=c(-2, 15)) + 
  labs(title="", x="Sentence Length", y="Written - Spoken", color="Head-finality") + 
  gg_theme()  + 
  theme(legend.position="bottom")

ggsave("plots/model_predictions_headedness_diff.png", plot=model.headedness.diff.g, device="png", dpi = 300, width=10)
model.headedness.diff.g

```

```{r merge_and_diff_headANDent}

df.predict.headANDent.diff.spoken <- df.predict.headANDent %>% 
  ungroup() %>%
  filter(modality == "Spoken (YouTube)") %>% 
  select(-modality) %>% 
  rename(difference_predicted_spoken = difference_predicted) %>%
  mutate(entropy_scaled = round(entropy_scaled, 2))
  
df.predict.headANDent.diff.written <- df.predict.headANDent  %>% 
  ungroup() %>%
  filter(modality == "Written (UD)") %>%
  select(-modality) %>% 
  rename(difference_predicted_written = difference_predicted) %>%
  mutate(entropy_scaled = round(entropy_scaled, 2))

df.predict.headANDent.diff <- merge(df.predict.headANDent.diff.spoken, df.predict.headANDent.diff.written) %>%
  mutate(difference_score = difference_predicted_spoken - difference_predicted_written)

```

```{r graph_headedness_entropy__diff_mean}

model.headANDent.diff.mean.g <- ggplot(df.predict.headANDent.diff %>% filter(lengthCat == "Mean length"),
                          aes(x=entropy_scaled, y=difference_score))

# Prediction line plot
model.headANDent.diff.mean.g <- model.headANDent.diff.mean.g +
  geom_line(aes(x=(entropy_scaled), y=(difference_score), 
                color=headednessCat), linetype="dashed", alpha=0.9, size=1.5)

# Theme
model.headANDent.diff.mean.g <- model.headANDent.diff.mean.g +
  scale_color_manual(values=c("#61c75d", "#0086d6", "#001C49", "#61c75d", "#0086d6", "#001C49")) +
  #geom_hline(aes(yintercept=0), linetype="dashed", width=1.5) + 
  #coord_cartesian(ylim=c(-1, 3)) + 
  facet_wrap(.~lengthCat) +
  labs(title="", x="Entropy", y="Spoken - Written", color="Head-finality") + 
  gg_theme()  + 
  theme(legend.position="bottom")

ggsave("plots/model_predictions_headednessEntropy_mean.png", plot=model.headANDent.diff.mean.g, device="png", dpi = 300, width=10)
model.headANDent.diff.mean.g

```

```{r graph_headedness_entropy__diff_max}

model.headANDent.diff.max.g <- ggplot(df.predict.headANDent.diff %>% filter(lengthCat == "Max length"),
                          aes(x=entropy_scaled, y=difference_score))

# Prediction line plot
model.headANDent.diff.max.g <- model.headANDent.diff.max.g +
  geom_line(aes(x=(entropy_scaled), y=(difference_score), 
                color=headednessCat), linetype="dashed", alpha=0.9, size=1.5)

# Theme
model.headANDent.diff.max.g <- model.headANDent.diff.max.g +
  scale_color_manual(values=c("#61c75d", "#0086d6", "#001C49", "#61c75d", "#0086d6", "#001C49")) +
  #geom_hline(aes(yintercept=0), linetype="dashed", width=1.5) + 
  #coord_cartesian(ylim=c(-1, 3)) + 
  facet_wrap(.~lengthCat) +
  labs(title="", x="Entropy", y="Spoken - Written", color="Head-finality") + 
  gg_theme()  + 
  theme(legend.position="bottom")

ggsave("plots/model_predictions_headednessEntropy_max.png", plot=model.headANDent.diff.max.g, device="png", dpi = 300, width=10)
model.headANDent.diff.max.g

```

